{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Inverse Reinforcement Learning for Market Making (2024)\n",
    "\n",
    "This notebook contains the code for the paper _Adversarial Inverse Reinforcement Learning for Market Making_ published in the proceedings of the [ICAIF'24](https://ai-finance.org/) conference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "from imitation.algorithms.adversarial.airl import AIRL\n",
    "from imitation.data import rollout, serialize\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from imitation.util.util import make_vec_env\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import VecCheckNan\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "from src.lob.commissions import BitCommissions\n",
    "from src.lob.exchange import Exchange\n",
    "from src.lob.plots import set_plot_style, visualize_backtest\n",
    "from src.lob.traders import RLMarketMaker\n",
    "from src.lob.utils import get_lot_size, get_tick_size\n",
    "from src.rl.environments import LimitOrderBookGym\n",
    "from src.rl.experts import ExpertPolicyV1, RandomPolicyV1\n",
    "from src.rl.plotting import visualize_airl_train_stats\n",
    "from src.rl.rewards import NegativeRewardNet\n",
    "from src.rl.utils import load_model, save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot style\n",
    "set_plot_style()\n",
    "\n",
    "# Set device\n",
    "DEVICE = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set strict error checking\n",
    "th.autograd.set_detect_anomaly(True)\n",
    "np.seterr(all=\"raise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for the pseudo random generator\n",
    "# SEED = 1\n",
    "# SEED = 2\n",
    "# SEED = 3\n",
    "# SEED = 4\n",
    "SEED = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the paths\n",
    "PATH = \"~/Projects/airl-market-making/data/pricing/\"\n",
    "PATH_VOL_DISTR = \"~/Projects/airl-market-making/data/volume_distributions/\"\n",
    "PATH_ROLLOUTS = \"data/rollouts/rollouts_2024-01-20_18-33-28.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register custom vectorized environment\n",
    "\n",
    "In this section I load the limit order book gym environment and register it as a custom vectorized environment. This is necessary for the `stable-baselines3` library to work with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "EXCHANGE_NAME = \"BIT.COM\"\n",
    "SYMBOL = \"SOL-USDT\"\n",
    "TICK_SIZE = get_tick_size(EXCHANGE_NAME)  # Tick size of the limit order book\n",
    "LOT_SIZE = get_lot_size(EXCHANGE_NAME)  # Lot size of the limit order book\n",
    "DEPTH = 20  # Depth of the data to load to the limit order book (max 20)\n",
    "EXCHANGE_TRADER_ID = \"Exchange\"\n",
    "MAX_STEPS = 300  # Maximum number of steps in an episode\n",
    "TS_START = pd.Timestamp(\"2023-09-01 00:00:00\")  # Start of the episode\n",
    "TS_END = pd.Timestamp(\"2023-09-10 23:59:59\")  # End of the episode\n",
    "DETERMINISTIC = False  # Indicates whether to use a deterministic environment\n",
    "WIN = 0  # Window size for the features computation\n",
    "LOGGING = False  # Indicates whether to log events\n",
    "TS_SAVE = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")  # Ts for model saving\n",
    "LATENCY_COMP_PARAMS = {}  # Parameters for the stochastic backtest\n",
    "RNG = np.random.default_rng(seed=SEED)  # Random number generator\n",
    "traders = {}  # Dictionary of traders\n",
    "\n",
    "print(\"Timestamp for saving: \", TS_SAVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters for the RL agent\n",
    "rl_trader_id = \"RLMarketMaker\"\n",
    "com_model = BitCommissions(tier=5)\n",
    "volume = 100\n",
    "\n",
    "# Initialize the trader\n",
    "trader = RLMarketMaker(\n",
    "    id=rl_trader_id,\n",
    "    com_model=com_model,\n",
    "    volume=volume,\n",
    ")\n",
    "traders[rl_trader_id] = trader\n",
    "\n",
    "# Write a description of the experiment\n",
    "description = \"RL market maker simulation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters for the environment\n",
    "ID = \"LimitOrderBookGym-v1\"\n",
    "ENTRY_POINT = LimitOrderBookGym\n",
    "KWARGS = {\n",
    "    \"exchange_name\": EXCHANGE_NAME,\n",
    "    \"symbol_name\": SYMBOL,\n",
    "    \"tick_size\": TICK_SIZE,\n",
    "    \"lot_size\": LOT_SIZE,\n",
    "    \"depth\": DEPTH,\n",
    "    \"traders\": traders,\n",
    "    \"max_steps\": MAX_STEPS,\n",
    "    \"ts_start\": TS_START,\n",
    "    \"ts_end\": TS_END,\n",
    "    \"deterministic\": DETERMINISTIC,\n",
    "    \"win\": WIN,\n",
    "    \"path\": PATH,\n",
    "    \"path_vol_distr\": PATH_VOL_DISTR,\n",
    "    \"rl_trader_id\": rl_trader_id,\n",
    "    \"latency_comp_params\": LATENCY_COMP_PARAMS,\n",
    "    \"logging\": LOGGING,\n",
    "    \"ts_save\": TS_SAVE,\n",
    "    \"description\": description,\n",
    "    \"rng\": RNG,\n",
    "}\n",
    "\n",
    "# Register the environment\n",
    "gym.envs.register(\n",
    "    id=ID,\n",
    "    entry_point=ENTRY_POINT,\n",
    "    kwargs=KWARGS,\n",
    "    max_episode_steps=MAX_STEPS,\n",
    ")\n",
    "\n",
    "# Create the environment\n",
    "env = Monitor(gym.make(ID))\n",
    "\n",
    "# Save the saving ts\n",
    "ts_save = env.unwrapped.exchange.ts_save\n",
    "print(f\"Saving ts: {ts_save}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vectorized environment\n",
    "venv = make_vec_env(\n",
    "    ID,\n",
    "    rng=RNG,\n",
    "    n_envs=1,\n",
    "    post_wrappers=[\n",
    "        lambda env, _: RolloutInfoWrapper(env)\n",
    "    ],  # needed for computing rollouts later\n",
    "    parallel=False,\n",
    ")\n",
    "venv = VecCheckNan(venv, raise_exception=True)  # Check for NaN observations\n",
    "venv.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate rollouts with random and expert policies\n",
    "\n",
    "In this section I define an expert policy that will be used as a target of the imitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters for the rollout\n",
    "min_timesteps = None\n",
    "min_episodes = 1\n",
    "\n",
    "# Rollout the environment with a random policy\n",
    "rollouts = rollout.rollout(\n",
    "    None,  # Random policy\n",
    "    venv,\n",
    "    sample_until=rollout.make_sample_until(\n",
    "        min_timesteps=min_timesteps, min_episodes=min_episodes\n",
    "    ),\n",
    "    rng=RNG,\n",
    ")\n",
    "\n",
    "# Print the first rollout\n",
    "for i in range(len(rollouts[0].obs) - 1):\n",
    "    print(\"Observation: \", rollouts[0].obs[i])\n",
    "    print(\"Action: \", rollouts[0].acts[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the random policy\n",
    "random_policy = RandomPolicyV1(venv.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the random policy\n",
    "reward_random_policy, _ = evaluate_policy(\n",
    "    random_policy, env, 1, return_episode_rewards=True\n",
    ")\n",
    "print(\"Reward: \", np.mean(reward_random_policy))\n",
    "print(\"Std   : \", np.std(reward_random_policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the expert policy\n",
    "expert = ExpertPolicyV1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters for the rollout\n",
    "min_timesteps = None\n",
    "min_episodes = 1\n",
    "\n",
    "# Rollout the environment with the expert policy\n",
    "rollouts = rollout.rollout(\n",
    "    expert.predict,\n",
    "    venv,\n",
    "    sample_until=rollout.make_sample_until(\n",
    "        min_timesteps=min_timesteps, min_episodes=min_episodes\n",
    "    ),\n",
    "    rng=RNG,\n",
    ")\n",
    "\n",
    "# Print the first rollout\n",
    "for i in range(len(rollouts[0].obs) - 1):\n",
    "    state, act = rollouts[0].obs[i][0], rollouts[0].acts[i]\n",
    "    print(f\"State 0: {state: .3f} --> Action: {act}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the trajectories into transitions\n",
    "transitions = rollout.flatten_trajectories(rollouts)\n",
    "transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the expert\n",
    "reward_expert_policy, _ = evaluate_policy(\n",
    "    expert, venv, 1, return_episode_rewards=True\n",
    ")\n",
    "print(\"Reward: \", np.mean(reward_expert_policy))\n",
    "print(\"Std   : \", np.std(reward_expert_policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the expert trajectories\n",
    "# path = \"rollouts/rollouts_2024-01-20_18-33-28.pkl\"\n",
    "\n",
    "\n",
    "# If the rollouts file exists load the rollouts\n",
    "if os.path.exists(PATH_ROLLOUTS):\n",
    "    rollouts = serialize.load(PATH_ROLLOUTS)\n",
    "\n",
    "# Else, generate the rollouts\n",
    "else:\n",
    "    # Set the parameters for the rollout\n",
    "    min_timesteps = 45000 * 3 + 4500\n",
    "    min_episodes = None\n",
    "\n",
    "    # Rollout the environment with the expert policy\n",
    "    print(\"Generating rollouts...\")\n",
    "    rollouts = rollout.rollout(\n",
    "        expert.predict,\n",
    "        venv,\n",
    "        sample_until=rollout.make_sample_until(\n",
    "            min_timesteps=min_timesteps, min_episodes=min_episodes\n",
    "        ),\n",
    "        rng=RNG,\n",
    "    )\n",
    "\n",
    "    # Ensure the directory exists and save the rollouts\n",
    "    os.makedirs(\"data/rollouts\", exist_ok=True)\n",
    "    serialize.save(PATH_ROLLOUTS, rollouts)\n",
    "\n",
    "# Print the first rollout\n",
    "for i in range(len(rollouts[0].obs) - 1):\n",
    "    print(\"Observation: \", rollouts[0].obs[i])\n",
    "    print(\"Action: \", rollouts[0].acts[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Inverse Reinforcement Learning Agent\n",
    "\n",
    "In this section I develop a pipeline for training the adversarial inverse reinforcement learning agent. The goal is to learn the reward function of the expert policy by training of the discriminator network and the agent policy network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for PPO (generator)\n",
    "learning_rate = 0.001  # Learning rate, can be a function of progress\n",
    "batch_size = 60  # Mini batch size for each gradient update\n",
    "n_epochs = 10  # N of epochs when optimizing the surrogate loss\n",
    "\n",
    "gamma = 0.5  # Discount factor, focus on the current reward\n",
    "gae_lambda = 0  # Generalized advantage estimation\n",
    "clip_range = 0.1  # Clipping parameter\n",
    "ent_coef = 0.01  # Entropy coefficient for the loss calculation\n",
    "vf_coef = 0.5  # Value function coef. for the loss calculation\n",
    "max_grad_norm = 0.5  # The maximum value for the gradient clipping\n",
    "\n",
    "verbose = 0  # Verbosity level: 0 no output, 1 info, 2 debug\n",
    "normalize_advantage = True  # Whether to normalize or not the advantage\n",
    "\n",
    "clip_range_vf = None  # Clip for the value function\n",
    "use_sde = False  # Use State Dependent Exploration\n",
    "sde_sample_freq = -1  # SDE - noise matrix frequency (-1 = disable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters for the (negative) reward net\n",
    "use_state = True  # Current state is used for the reward\n",
    "use_action = True  # Current action is used for the reward\n",
    "use_next_state = False  # Next state is used for the reward\n",
    "use_done = False  # Done flag is used for the reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters for the AIRL trainer\n",
    "gen_replay_buffer_capacity = None\n",
    "allow_variable_horizon = True\n",
    "\n",
    "disc_opt_kwargs = {\n",
    "    \"lr\": 0.001,\n",
    "}\n",
    "policy_kwargs = {\"use_expln\": True}  # Fixing the issue with the NaNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='orange'>**Warning:**</font> Be careful with the settings below and use the multiples of episode length (otherwise you might run into unexpected issues with variable horizons during training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of timesteps, batch size and number of disc updates\n",
    "\n",
    "# Total number of timesteps in the whole training\n",
    "total_timesteps = 3000 * 600\n",
    "\n",
    "# Generator\n",
    "gen_train_timesteps = 3000  # N steps in the environment per one round\n",
    "n_steps = gen_train_timesteps\n",
    "\n",
    "# Discriminator batches\n",
    "demo_minibatch_size = 60  # N samples in minibatch for one discrim. update\n",
    "demo_batch_size = 300 * 10  # N samples in the batch of expert data (batch)\n",
    "n_disc_updates_per_round = 4  # N discriminator updates per one round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the learner policy\n",
    "learner = PPO(\n",
    "    env=venv,\n",
    "    policy=MlpPolicy,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    learning_rate=learning_rate,\n",
    "    n_steps=n_steps,\n",
    "    batch_size=batch_size,\n",
    "    n_epochs=n_epochs,\n",
    "    gamma=gamma,\n",
    "    gae_lambda=gae_lambda,\n",
    "    clip_range=clip_range,\n",
    "    clip_range_vf=clip_range_vf,\n",
    "    normalize_advantage=normalize_advantage,\n",
    "    ent_coef=ent_coef,\n",
    "    vf_coef=vf_coef,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    use_sde=use_sde,\n",
    "    sde_sample_freq=sde_sample_freq,\n",
    "    verbose=verbose,\n",
    "    seed=SEED,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the custom reward network\n",
    "reward_net = NegativeRewardNet(\n",
    "    observation_space=venv.observation_space,\n",
    "    action_space=venv.action_space,\n",
    "    use_state=use_state,\n",
    "    use_action=use_action,\n",
    "    use_next_state=use_next_state,\n",
    "    use_done=use_done,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the AIRL trainer\n",
    "airl_trainer = AIRL(\n",
    "    demonstrations=rollouts,\n",
    "    demo_batch_size=demo_batch_size,\n",
    "    demo_minibatch_size=demo_minibatch_size,\n",
    "    n_disc_updates_per_round=n_disc_updates_per_round,\n",
    "    gen_train_timesteps=gen_train_timesteps,\n",
    "    gen_replay_buffer_capacity=gen_replay_buffer_capacity,\n",
    "    venv=venv,\n",
    "    gen_algo=learner,\n",
    "    reward_net=reward_net,\n",
    "    allow_variable_horizon=allow_variable_horizon,\n",
    "    disc_opt_kwargs=disc_opt_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the policy before training\n",
    "venv.seed(SEED)\n",
    "learner_rewards_before_training, _ = evaluate_policy(\n",
    "    learner, venv, 1, return_episode_rewards=True\n",
    ")\n",
    "print(\"Mean: \", np.mean(learner_rewards_before_training))\n",
    "print(\"Std: \", np.std(learner_rewards_before_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize actions of the policy before training\n",
    "for _ in range(1):\n",
    "    obs = venv.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        print(obs)\n",
    "        action, _ = learner.predict(obs, deterministic=True)\n",
    "        print(action)\n",
    "        print()\n",
    "        obs, _, done, _ = venv.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "airl_trainer.train(total_timesteps=total_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the policy after training\n",
    "venv.seed(SEED)\n",
    "learner_rewards_after_training, _ = evaluate_policy(\n",
    "    learner, venv, 5, return_episode_rewards=True\n",
    ")\n",
    "print(\"Mean: \", np.mean(learner_rewards_after_training))\n",
    "print(\"Std: \", np.std(learner_rewards_after_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize actions of the policy after training\n",
    "for _ in range(1):\n",
    "    obs = venv.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = learner.predict(obs, deterministic=True)\n",
    "        print(f\"Obs: {obs[0][0]} --> Action: {action}\")\n",
    "        obs, _, done, _ = venv.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the training log statistics\n",
    "stats = airl_trainer.logger._logger.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_airl_train_stats(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained model and stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "save_path = os.path.join(os.getcwd(), \"models\")\n",
    "ts = airl_trainer.ts_now\n",
    "print(f\"Saving the model with timestamp: {ts}\")\n",
    "save_model(learner, reward_net, stats, save_path, ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the timestamp of the model to load\n",
    "# ts = \"2024-01-21_17-12-35\" # seed 1\n",
    "# ts = \"2024-01-22_18-03-01\" # seed 2\n",
    "# ts = \"2024-01-23_19-14-27\" # seed 3\n",
    "# ts = \"2024-01-24_09-40-47\" # seed 4\n",
    "ts = \"2024-01-24_22-39-37\"  # seed 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "load_path = os.path.join(os.getcwd(), \"models\")\n",
    "learner, reward_net, stats = load_model(load_path, ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the stats\n",
    "save_fig = True\n",
    "visualize_airl_train_stats(stats, save_fig=save_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the policy after training\n",
    "venv.seed(SEED)\n",
    "learner_rewards_after_training, _ = evaluate_policy(\n",
    "    learner, venv, 5, return_episode_rewards=True\n",
    ")\n",
    "print(\"Mean: \", np.mean(learner_rewards_after_training))\n",
    "print(\"Std: \", np.std(learner_rewards_after_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize actions of the policy after training\n",
    "for _ in range(1):\n",
    "    obs = venv.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = learner.predict(obs, deterministic=True)\n",
    "        print(f\"Obs: {obs[0][0]: .5f} --> Action: {action}\")\n",
    "        obs, _, done, _ = venv.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "EXCHANGE_NAME = \"BIT.COM\"\n",
    "SYMBOL = \"SOL-USDT\"\n",
    "PATH = \"~/Projects/thesis-market-making/reinforcement-learning/data/\"\n",
    "TICK_SIZE = get_tick_size(EXCHANGE_NAME)  # Tick size of the limit order book\n",
    "LOT_SIZE = get_lot_size(EXCHANGE_NAME)  # Lot size of the limit order book\n",
    "DEPTH = 20  # Depth of the data to load to the limit order book (max 20)\n",
    "EXCHANGE_TRADER_ID = \"Exchange\"\n",
    "MAX_STEPS = None  # Maximum number of steps in an episode\n",
    "TS_START = pd.Timestamp(\"2023-09-11 00:00:00\")  # Start of the episode\n",
    "TS_END = pd.Timestamp(\"2023-09-13 23:59:59\")  # End of the episode\n",
    "WIN = 0  # Window size for the features computation\n",
    "LOGGING = False  # Indicates whether to log events\n",
    "LATENCY_COMP_PARAMS = {\n",
    "    0: {\"prob\": 0.9, \"divisor\": 1},\n",
    "    1: {\"prob\": 0.9, \"divisor\": 1},\n",
    "    2: {\"prob\": 0.9, \"divisor\": 1},\n",
    "    3: {\"prob\": 0.9, \"divisor\": 1},\n",
    "}  # Latency compensation parameters for the stochastic backtest\n",
    "RNG = np.random.default_rng(seed=SEED)  # Random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the limit order book and traders\n",
    "start = time.time()\n",
    "traders = {}\n",
    "\n",
    "# Behavior cloning agent\n",
    "rl_trader_id = \"RLMarketMaker\"\n",
    "com_model = BitCommissions(tier=5)\n",
    "volume = 100\n",
    "trader = RLMarketMaker(\n",
    "    id=rl_trader_id,\n",
    "    com_model=com_model,\n",
    "    volume=volume,\n",
    "    policy=learner.policy,\n",
    ")\n",
    "traders[rl_trader_id] = trader\n",
    "\n",
    "description = \"AIRL agent.\"\n",
    "\n",
    "# Initialize the exchange\n",
    "exchange = Exchange(\n",
    "    exchange_name=EXCHANGE_NAME,\n",
    "    symbol_name=SYMBOL,\n",
    "    tick_size=TICK_SIZE,\n",
    "    lot_size=LOT_SIZE,\n",
    "    depth=DEPTH,\n",
    "    traders=traders,\n",
    "    max_steps=MAX_STEPS,\n",
    "    ts_start=TS_START,\n",
    "    ts_end=TS_END,\n",
    "    win=WIN,\n",
    "    path=PATH,\n",
    "    rl_trader_id=rl_trader_id,\n",
    "    latency_comp_params=LATENCY_COMP_PARAMS,\n",
    "    logging=LOGGING,\n",
    "    ts_save=TS_SAVE,\n",
    "    description=description,\n",
    "    rng=RNG,\n",
    ")\n",
    "end = round(time.time() - start, 2)\n",
    "print(f\"Time taken for initialization of the exchange: {end} sec.\")\n",
    "\n",
    "# Run the exchange simulation\n",
    "start = time.time()\n",
    "exchange.run()\n",
    "end = round(time.time() - start, 2)\n",
    "print(f\"Time taken for running the exchange: {end} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = exchange.stats[\"ts\"]\n",
    "trader_stats = traders[rl_trader_id].stats\n",
    "initial_cost = 20.5 * volume * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_backtest(timestamps, trader_stats, initial_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airl-market-making-BPwY8dy3-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
