{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Inverse Reinforcement Learning for Market Making (2024)\n",
    "\n",
    "This notebook contains the code for the paper _Adversarial Inverse Reinforcement Learning for Market Making_ published in the proceedings of the [ICAIF'24](https://ai-finance.org/) conference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add a more detailed description of the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "from imitation.algorithms.adversarial.airl import AIRL\n",
    "from imitation.data import rollout, serialize\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from imitation.util.util import make_vec_env\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import VecCheckNan\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.data import download_data, get_list_of_dates_between\n",
    "from src.lob.commissions import BitCommissions\n",
    "from src.lob.exchange import Exchange\n",
    "from src.lob.plots import set_plot_style, visualize_backtest\n",
    "from src.lob.traders import RLMarketMaker\n",
    "from src.lob.utils import get_lot_size, get_tick_size\n",
    "from src.rl.environments import LimitOrderBookGym\n",
    "from src.rl.experts import ExpertPolicyV1, RandomPolicyV1\n",
    "from src.rl.plotting import visualize_airl_train_stats\n",
    "from src.rl.rewards import NegativeRewardNet\n",
    "from src.rl.utils import load_model, save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set plot style\n",
    "set_plot_style()\n",
    "\n",
    "# Set device\n",
    "DEVICE = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set strict error checking\n",
    "th.autograd.set_detect_anomaly(True)\n",
    "np.seterr(all=\"raise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for the pseudo random generator\n",
    "# SEED = 1\n",
    "# SEED = 2\n",
    "# SEED = 3\n",
    "# SEED = 4\n",
    "SEED = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the (relative) paths to data directories\n",
    "PATH = \"data/pricing/\"\n",
    "PATH_VOL_DISTR = \"data/volume_distributions/\"\n",
    "PATH_ROLLOUTS = \"data/rollouts/\"\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(PATH, exist_ok=True)\n",
    "os.makedirs(PATH_VOL_DISTR, exist_ok=True)\n",
    "os.makedirs(PATH_ROLLOUTS, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable/disable long print outputs\n",
    "PRINT = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: LOB data preprocessing\n",
    "\n",
    "In this section, we download and preprocess the limit order book and trades data for the training and testing of the AIRL market making agent.\n",
    "\n",
    "<font color='orange'>**Remark:**</font> As discussed in the paper, for the research and demonstration of the AIRL algorithm we get the data from the [Crypto Lake](https://crypto-lake.com/) data provider. For the SOL-USD there is data available from the smaller crypto exchange BIT.COM. Note that since the focus of the paper was the demonstration of the new AIRL approach, we only do a simple preprocessing of the data. For a real-world market making application, more sophisticated data preprocessing and live exchange analysis (e.g. real exchange volumes, detection of fake trades, latency measurements, API limits,  etc.) would be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Data download\n",
    "\n",
    "In this subsection, the limit order book and trade data are downloaded from the [Crypto Lake](https://crypto-lake.com/) data provider.\n",
    "\n",
    "<font color='orange'>**Remark:**</font> In order to download the data, it is required to have an active [subscription](https://crypto-lake.com/subscribe/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "exchange = \"BIT.COM\"\n",
    "symbol = \"SOL-USDT\"\n",
    "start_date = datetime(2023, 9, 1)\n",
    "end_date = datetime(2023, 9, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of dates\n",
    "dates = get_list_of_dates_between(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data\n",
    "for date in tqdm(dates):\n",
    "    download_data(date, symbol, exchange, \"book\", PATH)  # LOB data\n",
    "    download_data(date, symbol, exchange, \"trades\", PATH)  # Trade data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Visualize the bid-ask prices\n",
    "\n",
    "In this subsection, I visualize the bid-ask prices for different limit order book levels. As can be seen from the visualizations below, there are multiple timestamps with wide quoted spreads.\n",
    "\n",
    "These timestamps are filtered out from the dataset in the next subsection to worsen the market maker's performance. A more detailed analysis would be needed to determine whether such spreads could be captured by the market maker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "exchange = \"BIT.COM\"\n",
    "symbol = \"SOL-USDT\"\n",
    "start_date = datetime(2023, 9, 1)\n",
    "end_date = datetime(2023, 9, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the order book data from parquet files\n",
    "ob_prefix = f\"{exchange}_{symbol}_order_book\"\n",
    "\n",
    "# Generate a list of dates\n",
    "dates = get_list_of_dates_between(start_date, end_date)\n",
    "\n",
    "# Create a single joined dataframe with order book data\n",
    "df_joined = None\n",
    "for date in dates:\n",
    "    file_name = f\"{ob_prefix}_{date.strftime('%Y_%m_%d')}_original.parquet\"\n",
    "    df = pd.read_parquet(os.path.join(PATH, file_name))\n",
    "    if df_joined is None:\n",
    "        df_joined = df\n",
    "    else:\n",
    "        df_joined = pd.concat([df_joined, df])\n",
    "\n",
    "df_joined.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the bid-ask prices for each limit order book level\n",
    "for level in range(4):\n",
    "    fig = plt.figure(figsize=(14, 5))\n",
    "    plt.plot(df_joined[f\"bid_{level}_price\"], label=f\"Bid level {level}\")\n",
    "    plt.plot(df_joined[f\"ask_{level}_price\"], label=f\"Ask level {level}\")\n",
    "    plt.title(f\"Bid-ask prices for lob level {level}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Data cleaning\n",
    "\n",
    "In this section, we filter out the timestamps with the outlier spreads visualized in the previous section. Since the focus of the paper is on the market making strategy quoting limit orders, this assumption does not improve the backtest performance as the market making agent does not have the possibility to quote limit orders at the prices with these large spreads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = None\n",
    "\n",
    "for date in dates:\n",
    "    # Load the dataframe\n",
    "    file_name = f\"{ob_prefix}_{date.strftime('%Y_%m_%d')}_original.parquet\"\n",
    "    df = pd.read_parquet(os.path.join(PATH, file_name))\n",
    "\n",
    "    # Print the number of rows to be filtered\n",
    "    threshold = 30\n",
    "    a = df[\"ask_0_price\"] > threshold\n",
    "    b = df[\"ask_1_price\"] > threshold\n",
    "    c = df[\"ask_2_price\"] > threshold\n",
    "    print(\n",
    "        f\"Date: {date} -\",\n",
    "        f\"Rows to be filtered: {df[a | b | c].shape[0]} out of {df.shape[0]}\",\n",
    "        f\"({df[a | b | c].shape[0] / df.shape[0] * 100:.2f}%)\",\n",
    "    )\n",
    "\n",
    "    # Remove outliers\n",
    "    new_df = df[df[\"ask_0_price\"] < threshold]\n",
    "    new_df = new_df[new_df[\"ask_1_price\"] < threshold]\n",
    "    new_df = new_df[new_df[\"ask_2_price\"] < threshold]\n",
    "    new_df = new_df[new_df[\"ask_3_price\"] < threshold]\n",
    "\n",
    "    # Save the cleaned dataframe\n",
    "    new_file_name = f\"{ob_prefix}_{date.strftime('%Y_%m_%d')}.parquet\"\n",
    "    new_df.to_parquet(os.path.join(PATH, new_file_name))\n",
    "\n",
    "    # Join the dataframes\n",
    "    if df_joined is None:\n",
    "        df_joined = new_df\n",
    "    else:\n",
    "        df_joined = pd.concat([df_joined, new_df])\n",
    "\n",
    "df_joined.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the bid-ask prices for each limit order book level (after cleaning)\n",
    "for level in range(4):\n",
    "    fig = plt.figure(figsize=(14, 5))\n",
    "    plt.plot(df_joined[f\"bid_{level}_price\"], label=f\"Bid level {level}\")\n",
    "    plt.plot(df_joined[f\"ask_{level}_price\"], label=f\"Ask level {level}\")\n",
    "    plt.title(f\"Bid-ask prices for lob level {level}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outlier spreads are now removed from the data and the cleaned data are used for the training of the market making agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Generate volume distributions\n",
    "\n",
    "In this subsection, we generate the empirical volume distributions at each limit order book level. These empirical volume distributions are later used in the stochastic backtest simulator to sample the volumes of the penalizing front-running orders which worsen the priority of the market making agent's orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the new dates (Use only insample data for training)\n",
    "start_date = datetime(2023, 9, 1)\n",
    "end_date = datetime(2023, 9, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the volume distributions directory exists\n",
    "os.makedirs(PATH_VOL_DISTR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the order book data from parquet files\n",
    "ob_prefix = f\"{exchange}_{symbol}_order_book\"\n",
    "\n",
    "# Generate a list of dates\n",
    "dates = get_list_of_dates_between(start_date, end_date)\n",
    "\n",
    "# Create a single joined dataframe with order book data\n",
    "df_joined = None\n",
    "for date in dates:\n",
    "    file_name = f\"{ob_prefix}_{date.strftime('%Y_%m_%d')}.parquet\"\n",
    "    df = pd.read_parquet(os.path.join(PATH, file_name))\n",
    "    if df_joined is None:\n",
    "        df_joined = df\n",
    "    else:\n",
    "        df_joined = pd.concat([df_joined, df])\n",
    "\n",
    "df_joined.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level in range(3):\n",
    "    # Join the bid and ask volumes\n",
    "    vols_level = list(df_joined[f\"bid_{level}_size\"].values) + list(\n",
    "        df_joined[f\"ask_{level}_size\"].values\n",
    "    )\n",
    "\n",
    "    # Visualize the volume distribution\n",
    "    fig = plt.figure(figsize=(14, 5))\n",
    "    plt.hist(vols_level, bins=100, log=True)\n",
    "    plt.xlabel(\"Volume (SOL)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"Volume distribution for level {level} (log scale)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and save the volume distributions. This distributions are later\n",
    "# loaded and sampled from in the stochastical backtests.\n",
    "for level in range(3):\n",
    "    # Join the bid and ask volumes\n",
    "    vols_level = pd.Series(\n",
    "        list(df_joined[f\"bid_{level}_size\"].values)\n",
    "        + list(df_joined[f\"ask_{level}_size\"].values)\n",
    "    )\n",
    "\n",
    "    # Save the volume distribution\n",
    "    save_path = os.path.join(PATH_VOL_DISTR, f\"volumes_level_{level}.pkl\")\n",
    "    vols_level.to_pickle(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Pre-generate expert trajectories\n",
    "\n",
    "In the second section, we pre-generate the expert trajectories using the previously downloaded limit order book data. The expert trajectories are generated by simulating the expert-like market making agent's actions in the backtest simulator. These expert trajectories are later used in the adversarial inverse reinforcement learning algorithm to train the discriminator (or reward neural network) for distinguishing the expert trajectories from the adversarial agent's trajectories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Register custom vectorized environment\n",
    "\n",
    "In this section, I load our custom limit order book gym environment and register it as a custom vectorized environment. This is necessary for the `stable-baselines3` library to work with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp for saving:  2024-11-03_22-54-03\n"
     ]
    }
   ],
   "source": [
    "# Set the parameters\n",
    "EXCHANGE_NAME = \"BIT.COM\"\n",
    "SYMBOL = \"SOL-USDT\"\n",
    "TICK_SIZE = get_tick_size(EXCHANGE_NAME)  # Tick size of the limit order book\n",
    "LOT_SIZE = get_lot_size(EXCHANGE_NAME)  # Lot size of the limit order book\n",
    "DEPTH = 20  # Depth of the data to load to the limit order book (max 20)\n",
    "EXCHANGE_TRADER_ID = \"Exchange\"\n",
    "MAX_STEPS = 300  # Maximum number of steps in an episode\n",
    "TS_START = pd.Timestamp(\"2023-09-01 00:00:00\")  # Start of the episode\n",
    "TS_END = pd.Timestamp(\"2023-09-10 23:59:59\")  # End of the episode\n",
    "DETERMINISTIC = False  # Indicates whether to use a deterministic environment\n",
    "WIN = 0  # Window size for the features computation\n",
    "LOGGING = False  # Indicates whether to log events\n",
    "TS_SAVE = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")  # Ts for model saving\n",
    "LATENCY_COMP_PARAMS = {}  # Parameters for the stochastic backtest\n",
    "RNG = np.random.default_rng(seed=SEED)  # Random number generator\n",
    "traders = {}  # Dictionary of traders\n",
    "\n",
    "print(\"Timestamp for saving: \", TS_SAVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters for the RL agent\n",
    "rl_trader_id = \"RLMarketMaker\"\n",
    "com_model = BitCommissions(tier=5)\n",
    "volume = 100\n",
    "\n",
    "# Initialize the trader\n",
    "trader = RLMarketMaker(\n",
    "    id=rl_trader_id,\n",
    "    com_model=com_model,\n",
    "    volume=volume,\n",
    ")\n",
    "traders[rl_trader_id] = trader\n",
    "\n",
    "# Write a description of the experiment\n",
    "description = \"RL market maker simulation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ts: 2024-11-03_22-54-03\n"
     ]
    }
   ],
   "source": [
    "# Set the parameters for the environment\n",
    "ID = \"LimitOrderBookGym-v1\"\n",
    "ENTRY_POINT = LimitOrderBookGym\n",
    "KWARGS = {\n",
    "    \"exchange_name\": EXCHANGE_NAME,\n",
    "    \"symbol_name\": SYMBOL,\n",
    "    \"tick_size\": TICK_SIZE,\n",
    "    \"lot_size\": LOT_SIZE,\n",
    "    \"depth\": DEPTH,\n",
    "    \"traders\": traders,\n",
    "    \"max_steps\": MAX_STEPS,\n",
    "    \"ts_start\": TS_START,\n",
    "    \"ts_end\": TS_END,\n",
    "    \"deterministic\": DETERMINISTIC,\n",
    "    \"win\": WIN,\n",
    "    \"path\": PATH,\n",
    "    \"path_vol_distr\": PATH_VOL_DISTR,\n",
    "    \"rl_trader_id\": rl_trader_id,\n",
    "    \"latency_comp_params\": LATENCY_COMP_PARAMS,\n",
    "    \"logging\": LOGGING,\n",
    "    \"ts_save\": TS_SAVE,\n",
    "    \"description\": description,\n",
    "    \"rng\": RNG,\n",
    "}\n",
    "\n",
    "# Register the environment\n",
    "gym.envs.register(\n",
    "    id=ID,\n",
    "    entry_point=ENTRY_POINT,\n",
    "    kwargs=KWARGS,\n",
    "    max_episode_steps=MAX_STEPS,\n",
    ")\n",
    "\n",
    "# Create the environment\n",
    "env = Monitor(gym.make(ID))\n",
    "\n",
    "# Save the saving ts\n",
    "ts_save = env.unwrapped.exchange.ts_save\n",
    "print(f\"Saving ts: {ts_save}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -0.12282108, -0.11026608, -0.12069577,  0.12282108,\n",
       "         0.14267993,  0.15329003,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the vectorized environment\n",
    "venv = make_vec_env(\n",
    "    ID,\n",
    "    rng=RNG,\n",
    "    n_envs=1,\n",
    "    post_wrappers=[\n",
    "        lambda env, _: RolloutInfoWrapper(env)\n",
    "    ],  # needed for computing rollouts later\n",
    "    parallel=False,\n",
    ")\n",
    "venv = VecCheckNan(venv, raise_exception=True)  # Check for NaN observations\n",
    "venv.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Generate rollouts with random and expert policies\n",
    "\n",
    "In this subsection, we define an expert policy that will be used as a target for the adversarial agent. We use this expert policy to generate the expert trajectories for the AIRL algorithm. These trajectories are saved to the `data/rollouts` directory and loaded before training to save time in the training process. Alternatively, you can also generate these during the AIRL training process but this will take more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters for the rollout\n",
    "min_timesteps = None\n",
    "min_episodes = 1\n",
    "\n",
    "# Rollout the environment with a random policy\n",
    "rollouts = rollout.rollout(\n",
    "    None,  # Random policy\n",
    "    venv,\n",
    "    sample_until=rollout.make_sample_until(\n",
    "        min_timesteps=min_timesteps, min_episodes=min_episodes\n",
    "    ),\n",
    "    rng=RNG,\n",
    ")\n",
    "\n",
    "# Print the first rollout\n",
    "if PRINT:\n",
    "    for i in range(len(rollouts[0].obs) - 1):\n",
    "        print(\"Observation: \", rollouts[0].obs[i])\n",
    "        print(\"Action: \", rollouts[0].acts[i])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward:  14.0\n",
      "Std:     0.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize the random policy\n",
    "random_policy = RandomPolicyV1(venv.action_space)\n",
    "\n",
    "# Evaluate the random policy\n",
    "reward_random_policy, _ = evaluate_policy(\n",
    "    random_policy, env, 1, return_episode_rewards=True\n",
    ")\n",
    "print(\"Reward: \", np.mean(reward_random_policy))\n",
    "print(\"Std:    \", np.std(reward_random_policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the expert policy\n",
    "expert = ExpertPolicyV1()\n",
    "\n",
    "# Set the parameters for the rollout\n",
    "min_timesteps = None\n",
    "min_episodes = 1\n",
    "\n",
    "# Rollout the environment with the expert policy\n",
    "rollouts = rollout.rollout(\n",
    "    expert.predict,\n",
    "    venv,\n",
    "    sample_until=rollout.make_sample_until(\n",
    "        min_timesteps=min_timesteps, min_episodes=min_episodes\n",
    "    ),\n",
    "    rng=RNG,\n",
    ")\n",
    "\n",
    "# Print the first rollout\n",
    "if PRINT:\n",
    "    for i in range(len(rollouts[0].obs) - 1):\n",
    "        state, act = rollouts[0].obs[i][0], rollouts[0].acts[i]\n",
    "        print(f\"State 0: {state: .3f} --> Action: {act}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the trajectories into transitions\n",
    "transitions = rollout.flatten_trajectories(rollouts)\n",
    "if PRINT:\n",
    "    print(\"Transitions: \", transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward:  300.0\n",
      "Std:     0.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the expert\n",
    "reward_expert_policy, _ = evaluate_policy(\n",
    "    expert, venv, 1, return_episode_rewards=True\n",
    ")\n",
    "print(\"Reward: \", np.mean(reward_expert_policy))\n",
    "print(\"Std:    \", np.std(reward_expert_policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or generate larger sample of expert trajectories.\n",
    "# These rollouts are later used to train the AIRL agent, in particular they are\n",
    "# used to train its discriminator.\n",
    "rollouts_file = os.path.join(PATH_ROLLOUTS, \"rollouts.pkl\")\n",
    "\n",
    "# If the rollouts file exists load the rollouts\n",
    "if os.path.exists(rollouts_file):\n",
    "    rollouts = serialize.load(rollouts_file)\n",
    "\n",
    "# Else, generate the rollouts\n",
    "else:\n",
    "    # Set the parameters for the rollout\n",
    "    min_timesteps = 45000 * 3 + 4500\n",
    "    min_episodes = None\n",
    "\n",
    "    # Rollout the environment with the expert policy\n",
    "    print(\"Generating rollouts... (Might take around 15 minutes.)\")\n",
    "    rollouts = rollout.rollout(\n",
    "        expert.predict,\n",
    "        venv,\n",
    "        sample_until=rollout.make_sample_until(\n",
    "            min_timesteps=min_timesteps, min_episodes=min_episodes\n",
    "        ),\n",
    "        rng=RNG,\n",
    "    )\n",
    "\n",
    "    # Ensure the directory exists and save the rollouts\n",
    "    serialize.save(rollouts_file, rollouts)\n",
    "\n",
    "# Print the first rollout\n",
    "if PRINT:\n",
    "    for i in range(len(rollouts[0].obs) - 1):\n",
    "        print(\"Observation: \", rollouts[0].obs[i])\n",
    "        print(\"Action: \", rollouts[0].acts[i])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Training of the AIRL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Initialize the AIRL agent\n",
    "\n",
    "In this section I develop a pipeline for training the adversarial inverse reinforcement learning agent. The goal is to learn the reward function of the expert policy by training of the discriminator network and the agent policy network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for PPO (generator)\n",
    "learning_rate = 0.001  # Learning rate, can be a function of progress\n",
    "batch_size = 60  # Mini batch size for each gradient update\n",
    "n_epochs = 10  # N of epochs when optimizing the surrogate loss\n",
    "\n",
    "gamma = 0.5  # Discount factor, focus on the current reward\n",
    "gae_lambda = 0  # Generalized advantage estimation\n",
    "clip_range = 0.1  # Clipping parameter\n",
    "ent_coef = 0.01  # Entropy coefficient for the loss calculation\n",
    "vf_coef = 0.5  # Value function coef. for the loss calculation\n",
    "max_grad_norm = 0.5  # The maximum value for the gradient clipping\n",
    "\n",
    "verbose = 0  # Verbosity level: 0 no output, 1 info, 2 debug\n",
    "normalize_advantage = True  # Whether to normalize or not the advantage\n",
    "\n",
    "clip_range_vf = None  # Clip for the value function\n",
    "use_sde = False  # Use State Dependent Exploration\n",
    "sde_sample_freq = -1  # SDE - noise matrix frequency (-1 = disable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters for the (negative) reward net\n",
    "use_state = True  # Current state is used for the reward\n",
    "use_action = True  # Current action is used for the reward\n",
    "use_next_state = False  # Next state is used for the reward\n",
    "use_done = False  # Done flag is used for the reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters for the AIRL trainer\n",
    "gen_replay_buffer_capacity = None\n",
    "allow_variable_horizon = True\n",
    "\n",
    "disc_opt_kwargs = {\n",
    "    \"lr\": 0.001,\n",
    "}\n",
    "policy_kwargs = {\"use_expln\": True}  # Fixing the issue with the NaNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='orange'>**Warning:**</font> Be careful with the settings below and use the multiples of episode length (otherwise you might run into unexpected issues with variable horizons during training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of timesteps, batch size and number of disc updates\n",
    "\n",
    "# Total number of timesteps in the whole training\n",
    "total_timesteps = 3000 * 600\n",
    "\n",
    "# Generator\n",
    "gen_train_timesteps = 3000  # N steps in the environment per one round\n",
    "n_steps = gen_train_timesteps\n",
    "\n",
    "# Discriminator batches\n",
    "demo_minibatch_size = 60  # N samples in minibatch for one discrim. update\n",
    "demo_batch_size = 300 * 10  # N samples in the batch of expert data (batch)\n",
    "n_disc_updates_per_round = 4  # N discriminator updates per one round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the learner policy\n",
    "learner = PPO(\n",
    "    env=venv,\n",
    "    policy=MlpPolicy,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    learning_rate=learning_rate,\n",
    "    n_steps=n_steps,\n",
    "    batch_size=batch_size,\n",
    "    n_epochs=n_epochs,\n",
    "    gamma=gamma,\n",
    "    gae_lambda=gae_lambda,\n",
    "    clip_range=clip_range,\n",
    "    clip_range_vf=clip_range_vf,\n",
    "    normalize_advantage=normalize_advantage,\n",
    "    ent_coef=ent_coef,\n",
    "    vf_coef=vf_coef,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    use_sde=use_sde,\n",
    "    sde_sample_freq=sde_sample_freq,\n",
    "    verbose=verbose,\n",
    "    seed=SEED,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the custom reward network\n",
    "reward_net = NegativeRewardNet(\n",
    "    observation_space=venv.observation_space,\n",
    "    action_space=venv.action_space,\n",
    "    use_state=use_state,\n",
    "    use_action=use_action,\n",
    "    use_next_state=use_next_state,\n",
    "    use_done=use_done,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the AIRL trainer\n",
    "airl_trainer = AIRL(\n",
    "    demonstrations=rollouts,\n",
    "    demo_batch_size=demo_batch_size,\n",
    "    demo_minibatch_size=demo_minibatch_size,\n",
    "    n_disc_updates_per_round=n_disc_updates_per_round,\n",
    "    gen_train_timesteps=gen_train_timesteps,\n",
    "    gen_replay_buffer_capacity=gen_replay_buffer_capacity,\n",
    "    venv=venv,\n",
    "    gen_algo=learner,\n",
    "    reward_net=reward_net,\n",
    "    allow_variable_horizon=allow_variable_horizon,\n",
    "    disc_opt_kwargs=disc_opt_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the policy before training\n",
    "venv.seed(SEED)\n",
    "learner_rewards_before_training, _ = evaluate_policy(\n",
    "    learner, venv, 1, return_episode_rewards=True\n",
    ")\n",
    "print(\"Mean: \", np.mean(learner_rewards_before_training))\n",
    "print(\"Std: \", np.std(learner_rewards_before_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize actions of the policy before training\n",
    "if PRINT:\n",
    "    for _ in range(1):\n",
    "        obs = venv.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            print(obs)\n",
    "            action, _ = learner.predict(obs, deterministic=True)\n",
    "            print(action)\n",
    "            print()\n",
    "            obs, _, done, _ = venv.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Train the AIRL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "airl_trainer.train(total_timesteps=total_timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Evaluate the trained AIRL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the policy after training\n",
    "venv.seed(SEED)\n",
    "learner_rewards_after_training, _ = evaluate_policy(\n",
    "    learner, venv, 5, return_episode_rewards=True\n",
    ")\n",
    "print(\"Mean: \", np.mean(learner_rewards_after_training))\n",
    "print(\"Std: \", np.std(learner_rewards_after_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize actions of the policy after training\n",
    "if PRINT:\n",
    "    for _ in range(1):\n",
    "        obs = venv.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = learner.predict(obs, deterministic=True)\n",
    "            print(f\"Obs: {obs[0][0]} --> Action: {action}\")\n",
    "            obs, _, done, _ = venv.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the training log statistics\n",
    "stats = airl_trainer.logger._logger.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_airl_train_stats(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Save the trained model and training statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "save_path = os.path.join(os.getcwd(), \"models\")\n",
    "ts = airl_trainer.ts_now\n",
    "print(f\"Saving the model with timestamp: {ts}\")\n",
    "save_model(learner, reward_net, stats, save_path, ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section X: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the timestamp of the model to load\n",
    "# ts = \"2024-01-21_17-12-35\" # seed 1\n",
    "# ts = \"2024-01-22_18-03-01\" # seed 2\n",
    "# ts = \"2024-01-23_19-14-27\" # seed 3\n",
    "# ts = \"2024-01-24_09-40-47\" # seed 4\n",
    "ts = \"2024-01-24_22-39-37\"  # seed 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "load_path = os.path.join(os.getcwd(), \"models\")\n",
    "learner, reward_net, stats = load_model(load_path, ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the stats\n",
    "save_fig = True\n",
    "visualize_airl_train_stats(stats, save_fig=save_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the policy after training\n",
    "venv.seed(SEED)\n",
    "learner_rewards_after_training, _ = evaluate_policy(\n",
    "    learner, venv, 5, return_episode_rewards=True\n",
    ")\n",
    "print(\"Mean: \", np.mean(learner_rewards_after_training))\n",
    "print(\"Std: \", np.std(learner_rewards_after_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize actions of the policy after training\n",
    "for _ in range(1):\n",
    "    obs = venv.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = learner.predict(obs, deterministic=True)\n",
    "        print(f\"Obs: {obs[0][0]: .5f} --> Action: {action}\")\n",
    "        obs, _, done, _ = venv.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "EXCHANGE_NAME = \"BIT.COM\"\n",
    "SYMBOL = \"SOL-USDT\"\n",
    "PATH = \"~/Projects/thesis-market-making/reinforcement-learning/data/\"\n",
    "TICK_SIZE = get_tick_size(EXCHANGE_NAME)  # Tick size of the limit order book\n",
    "LOT_SIZE = get_lot_size(EXCHANGE_NAME)  # Lot size of the limit order book\n",
    "DEPTH = 20  # Depth of the data to load to the limit order book (max 20)\n",
    "EXCHANGE_TRADER_ID = \"Exchange\"\n",
    "MAX_STEPS = None  # Maximum number of steps in an episode\n",
    "TS_START = pd.Timestamp(\"2023-09-11 00:00:00\")  # Start of the episode\n",
    "TS_END = pd.Timestamp(\"2023-09-13 23:59:59\")  # End of the episode\n",
    "WIN = 0  # Window size for the features computation\n",
    "LOGGING = False  # Indicates whether to log events\n",
    "LATENCY_COMP_PARAMS = {\n",
    "    0: {\"prob\": 0.9, \"divisor\": 1},\n",
    "    1: {\"prob\": 0.9, \"divisor\": 1},\n",
    "    2: {\"prob\": 0.9, \"divisor\": 1},\n",
    "    3: {\"prob\": 0.9, \"divisor\": 1},\n",
    "}  # Latency compensation parameters for the stochastic backtest\n",
    "RNG = np.random.default_rng(seed=SEED)  # Random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the limit order book and traders\n",
    "start = time.time()\n",
    "traders = {}\n",
    "\n",
    "# Behavior cloning agent\n",
    "rl_trader_id = \"RLMarketMaker\"\n",
    "com_model = BitCommissions(tier=5)\n",
    "volume = 100\n",
    "trader = RLMarketMaker(\n",
    "    id=rl_trader_id,\n",
    "    com_model=com_model,\n",
    "    volume=volume,\n",
    "    policy=learner.policy,\n",
    ")\n",
    "traders[rl_trader_id] = trader\n",
    "\n",
    "description = \"AIRL agent.\"\n",
    "\n",
    "# Initialize the exchange\n",
    "exchange = Exchange(\n",
    "    exchange_name=EXCHANGE_NAME,\n",
    "    symbol_name=SYMBOL,\n",
    "    tick_size=TICK_SIZE,\n",
    "    lot_size=LOT_SIZE,\n",
    "    depth=DEPTH,\n",
    "    traders=traders,\n",
    "    max_steps=MAX_STEPS,\n",
    "    ts_start=TS_START,\n",
    "    ts_end=TS_END,\n",
    "    win=WIN,\n",
    "    path=PATH,\n",
    "    rl_trader_id=rl_trader_id,\n",
    "    latency_comp_params=LATENCY_COMP_PARAMS,\n",
    "    logging=LOGGING,\n",
    "    ts_save=TS_SAVE,\n",
    "    description=description,\n",
    "    rng=RNG,\n",
    ")\n",
    "end = round(time.time() - start, 2)\n",
    "print(f\"Time taken for initialization of the exchange: {end} sec.\")\n",
    "\n",
    "# Run the exchange simulation\n",
    "start = time.time()\n",
    "exchange.run()\n",
    "end = round(time.time() - start, 2)\n",
    "print(f\"Time taken for running the exchange: {end} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = exchange.stats[\"ts\"]\n",
    "trader_stats = traders[rl_trader_id].stats\n",
    "initial_cost = 20.5 * volume * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_backtest(timestamps, trader_stats, initial_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airl-market-making-BPwY8dy3-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
