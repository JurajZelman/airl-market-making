{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Adversarial Inverse Reinforcement Learning for Market Making](.images/title_image.png)\n",
    "\n",
    "This notebook contains the code for the paper *Adversarial Inverse Reinforcement Learning for Market Making* (2024) [[arxiv](), [ACM]()] by Juraj Zelman (Richfox Capital &  ETH Zürich), Martin Stefanik (Richfox Capital &  ETH Zürich), Moritz Weiß (ETH Zürich) and Prof. Dr. Josef Teichmann (ETH Zürich). The paper was published and presented at the [ICAIF'24](https://ai-finance.org/) conference. The links for both affiliations: [Richfox Capital](https://www.richfox.com/) and [ETH Zürich (Dept. of Mathematics)](https://math.ethz.ch/).\n",
    "\n",
    "In the following sections, we demonstrate a pipeline for training of the [Adversarial Inverse Reinforcement Learning agent](https://imitation.readthedocs.io/en/latest/algorithms/airl.html) implemented in the [imitation](https://imitation.readthedocs.io/en/latest/index.html) package, The package is described in the paper [Imitation: Clean imitation learning implementations - Gleave et al. (2022)](https://arxiv.org/pdf/2211.11972). The goal is to learn the reward function of the expert policy by training of the discriminator network and the agent policy network by the Proximal Policy Optimization algorithm introduced in the paper [Proximal Policy Optimization Algorithms - Schulman et al. (2017)](https://arxiv.org/abs/1707.06347). For the PPO policy, we use the implementation from the [stable-baselines3](https://stable-baselines3.readthedocs.io/en/master/) library. This library is described in more detail in the paper [Stable-baselines3: Reliable reinforcement learning implementations - Raffin et al. (2021)](https://www.jmlr.org/papers/volume22/20-1364/20-1364.pdf). \n",
    "\n",
    "This notebook is structured as follows:\n",
    "- [**Section 1: Limit order book data preprocessing**](#section-1-limit-order-book-data-preprocessing) - In this section, the pricing data are queried, cleaned and additionally preprocessed for the training of the market making agent and backtesting.\n",
    "- [**Section 2: Pre-generate expert trajectories**](#section-2-pre-generate-expert-trajectories) - In this section, we initialize our custom limit order book environment and pre-generate expert trajectories for the training of the adversarial inverse reinforcement learning (AIRL) agent. This is to save computational resources during the training of the AIRL agent.\n",
    "- [**Section 3: Training of the AIRL agent**](#section-3-training-of-the-airl-agent) - This section contains the full training pipeline of the adversarial inverse reinforcement learning agent (hyperparameters, initialization, training and saving).\n",
    "- [**Section 4: Evaluation of the AIRL agent**](#section-4-evaluation-of-the-airl-agent)\n",
    "- [**Section 5: Backtests**](#section-5-backtests)\n",
    "- [**Section 6: Backtest visualizations**](#section-6-backtest-visualizations)\n",
    "- [**Section 7: Appendix (data analysis)**](#section-7-appendix-data-analysis)\n",
    "\n",
    "Firstly, let's start by importing the necessary libraries and with general setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "from imitation.algorithms.adversarial.airl import AIRL\n",
    "from imitation.data import rollout, serialize\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from imitation.util.util import make_vec_env\n",
    "from matplotlib.dates import DateFormatter\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import VecCheckNan\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.data import download_data, get_list_of_dates_between\n",
    "from src.lob.backtest_metrics import drawdowns\n",
    "from src.lob.commissions import BitCommissions\n",
    "from src.lob.exchange import Exchange\n",
    "from src.lob.plots import (\n",
    "    make_drawdown_plot,\n",
    "    make_plot,\n",
    "    set_plot_style,\n",
    "    visualize_backtest,\n",
    ")\n",
    "from src.lob.traders import PureMarketMaker, RLMarketMaker\n",
    "from src.lob.utils import get_lot_size, get_tick_size\n",
    "from src.rl.environments import LimitOrderBookGym\n",
    "from src.rl.experts import ExpertPolicy, RandomPolicy\n",
    "from src.rl.plotting import visualize_airl_train_stats\n",
    "from src.rl.rewards import NegativeRewardNet\n",
    "from src.rl.utils import load_model, save_model, send_notification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set style of plots\n",
    "set_plot_style()\n",
    "\n",
    "# Set strict error checking\n",
    "th.autograd.set_detect_anomaly(True)\n",
    "np.seterr(all=\"raise\")\n",
    "\n",
    "# Set device to GPU if available\n",
    "DEVICE = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set (relative) paths to directories\n",
    "PATH = \"data/pricing/\"\n",
    "PATH_MODELS = \"data/models\"\n",
    "PATH_ROLLOUTS = \"data/rollouts/\"\n",
    "PATH_VOL_DISTR = \"data/volume_distributions/\"\n",
    "PATH_AUTOMATED_BACKTESTS = \"data/automated_backtests\"\n",
    "PATH_FIGURES = \"figures/\"\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(PATH, exist_ok=True)\n",
    "os.makedirs(PATH_MODELS, exist_ok=True)\n",
    "os.makedirs(PATH_ROLLOUTS, exist_ok=True)\n",
    "os.makedirs(PATH_VOL_DISTR, exist_ok=True)\n",
    "os.makedirs(PATH_AUTOMATED_BACKTESTS, exist_ok=True)\n",
    "os.makedirs(PATH_FIGURES, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set printing/plotting options\n",
    "PRINT = False  # Enable/disable long print outputs\n",
    "SAVE_FIG = False  # Enable/disable saving of figures\n",
    "FIG_SIZE = (14, 5)  # Set size of figures\n",
    "\n",
    "# Set custom color codes\n",
    "COLOR_GREEN, COLOR_RED = \"#13961A\", \"#EB5C14\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for the random generator\n",
    "# SEED = 1\n",
    "# SEED = 2\n",
    "# SEED = 3\n",
    "# SEED = 4\n",
    "SEED = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Limit order book data preprocessing\n",
    "\n",
    "In this first section, we download and preprocess the limit order book and trades data for the training and testing of the AIRL market making agent.\n",
    "\n",
    "<font color='orange'>**Remark:**</font> As discussed in the paper, for this public research and demonstration of the AIRL algorithm we get the data from the [Crypto Lake](https://crypto-lake.com/) data provider. For the SOL-USD there is data available from the smaller crypto exchange BIT.COM. Note that since the focus of the paper was the demonstration of the new AIRL approach, we only do a simple preprocessing of the data. For a real-world market making application, more sophisticated data preprocessing and live exchange analysis (e.g. real exchange volumes, detection of fake trades, latency measurements, API limits,  etc.) would be necessary and was beyond the scope of the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Data download\n",
    "\n",
    "Firstly, the limit order book and trade data are downloaded from the [Crypto Lake](https://crypto-lake.com/) data provider.\n",
    "\n",
    "<font color='orange'>**Remark:**</font> In order to download the data, it is required to have an active [subscription](https://crypto-lake.com/subscribe/) and the associated AWS credentials file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "exchange = \"BIT.COM\"\n",
    "symbol = \"SOL-USDT\"\n",
    "start_date = datetime(2023, 9, 1)\n",
    "end_date = datetime(2023, 9, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of dates to download\n",
    "dates = get_list_of_dates_between(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query data\n",
    "for date in tqdm(dates):\n",
    "    download_data(date, symbol, exchange, \"book\", PATH)  # LOB data\n",
    "    download_data(date, symbol, exchange, \"trades\", PATH)  # Trade data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Visualize the bid-ask prices\n",
    "\n",
    "In this subsection, we visualize the bid-ask prices for different limit order book levels. As can be seen from the visualizations below, there are multiple timestamps with wide quoted spreads.\n",
    "\n",
    "These timestamps are filtered out from the dataset in the next subsection so that the market maker does not profit from these (to be safe we choose the assumption that these were not capturable). This assumption can potentially only worsen the market maker's performance in our backtests. A more detailed analysis would be needed to determine whether such spreads could be captured by the market maker in live trading but this is beyond the scope of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "exchange = \"BIT.COM\"\n",
    "symbol = \"SOL-USDT\"\n",
    "start_date = datetime(2023, 9, 1)\n",
    "end_date = datetime(2023, 9, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load order book data from parquet files\n",
    "ob_prefix = f\"{exchange}_{symbol}_order_book\"\n",
    "\n",
    "# Generate a list of dates\n",
    "dates = get_list_of_dates_between(start_date, end_date)\n",
    "\n",
    "# Create a single joined dataframe with order book data\n",
    "df_joined = None\n",
    "for date in dates:\n",
    "    file_name = f\"{ob_prefix}_{date.strftime('%Y_%m_%d')}_original.parquet\"\n",
    "    df = pd.read_parquet(os.path.join(PATH, file_name))\n",
    "    if df_joined is None:\n",
    "        df_joined = df\n",
    "    else:\n",
    "        df_joined = pd.concat([df_joined, df])\n",
    "\n",
    "df_joined.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize bid-ask prices for each limit order book level\n",
    "for level in range(4):\n",
    "    fig = plt.figure(figsize=FIG_SIZE)\n",
    "    plt.plot(df_joined[f\"bid_{level}_price\"], label=f\"Bid level {level}\")\n",
    "    plt.plot(df_joined[f\"ask_{level}_price\"], label=f\"Ask level {level}\")\n",
    "    plt.title(f\"Bid-ask prices for lob level {level}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Data cleaning\n",
    "\n",
    "In the following, we filter out the timestamps with the outlier spreads visualized in the previous section. The number of timestamps filtered for each day is printed below.\n",
    "\n",
    "Since the focus of the paper is on the market making strategy quoting limit orders, this assumption does not improve the backtest performance as the market making agent does not have the possibility to quote limit orders at the prices with these large spreads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and join pricing dataframes\n",
    "df_joined = None\n",
    "\n",
    "for date in dates:\n",
    "    # Load a dataframe\n",
    "    file_name = f\"{ob_prefix}_{date.strftime('%Y_%m_%d')}_original.parquet\"\n",
    "    df = pd.read_parquet(os.path.join(PATH, file_name))\n",
    "\n",
    "    # Print the number of rows to be filtered\n",
    "    threshold = 30\n",
    "    a = df[\"ask_0_price\"] > threshold\n",
    "    b = df[\"ask_1_price\"] > threshold\n",
    "    c = df[\"ask_2_price\"] > threshold\n",
    "    print(\n",
    "        f\"Date: {date} -\",\n",
    "        f\"Rows to be filtered: {df[a | b | c].shape[0]} out of {df.shape[0]}\",\n",
    "        f\"({df[a | b | c].shape[0] / df.shape[0] * 100:.2f}%)\",\n",
    "    )\n",
    "\n",
    "    # Remove outliers\n",
    "    new_df = df[df[\"ask_0_price\"] < threshold]\n",
    "    new_df = new_df[new_df[\"ask_1_price\"] < threshold]\n",
    "    new_df = new_df[new_df[\"ask_2_price\"] < threshold]\n",
    "    new_df = new_df[new_df[\"ask_3_price\"] < threshold]\n",
    "\n",
    "    # Save the cleaned dataframe\n",
    "    new_file_name = f\"{ob_prefix}_{date.strftime('%Y_%m_%d')}.parquet\"\n",
    "    new_df.to_parquet(os.path.join(PATH, new_file_name))\n",
    "\n",
    "    # Join the dataframes\n",
    "    if df_joined is None:\n",
    "        df_joined = new_df\n",
    "    else:\n",
    "        df_joined = pd.concat([df_joined, new_df])\n",
    "\n",
    "df_joined.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize bid-ask prices for each limit order book level (after cleaning)\n",
    "for level in range(4):\n",
    "    fig = plt.figure(figsize=(14, 5))\n",
    "    plt.plot(df_joined[f\"bid_{level}_price\"], label=f\"Bid level {level}\")\n",
    "    plt.plot(df_joined[f\"ask_{level}_price\"], label=f\"Ask level {level}\")\n",
    "    plt.title(f\"Bid-ask prices for lob level {level}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outlier spreads are now removed from the data and the cleaned data are used for the training of the market making agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Generate volume distributions\n",
    "\n",
    "Here, we generate empirical volume distributions at each limit order book level. As described in the paper, these empirical volume distributions are used in the stochastic backtest simulator to sample volumes of penalizing front-running orders which worsen order book priority of the market making agent's orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set new dates (use only insample data for training)\n",
    "start_date = datetime(2023, 9, 1)\n",
    "end_date = datetime(2023, 9, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load order book data from parquet files\n",
    "ob_prefix = f\"{exchange}_{symbol}_order_book\"\n",
    "\n",
    "# Generate a list of dates\n",
    "dates = get_list_of_dates_between(start_date, end_date)\n",
    "\n",
    "# Create a single joined dataframe with order book data\n",
    "df_joined = None\n",
    "for date in dates:\n",
    "    file_name = f\"{ob_prefix}_{date.strftime('%Y_%m_%d')}.parquet\"\n",
    "    df = pd.read_parquet(os.path.join(PATH, file_name))\n",
    "    if df_joined is None:\n",
    "        df_joined = df\n",
    "    else:\n",
    "        df_joined = pd.concat([df_joined, df])\n",
    "\n",
    "df_joined.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize volume distribution for each limit order book level\n",
    "for level in range(3):\n",
    "    # Join the bid and ask volumes\n",
    "    vols_level = list(df_joined[f\"bid_{level}_size\"].values) + list(\n",
    "        df_joined[f\"ask_{level}_size\"].values\n",
    "    )\n",
    "\n",
    "    # Visualize the volume distribution\n",
    "    fig = plt.figure(figsize=FIG_SIZE)\n",
    "    plt.hist(vols_level, bins=100, log=True)\n",
    "    plt.xlabel(\"Volume (SOL)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"Volume distribution for level {level} (log scale)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and save volume distributions. These distributions are later\n",
    "# loaded and sampled from in stochastical backtests.\n",
    "for level in range(3):\n",
    "    # Join bid and ask volumes\n",
    "    vols_level = pd.Series(\n",
    "        list(df_joined[f\"bid_{level}_size\"].values)\n",
    "        + list(df_joined[f\"ask_{level}_size\"].values)\n",
    "    )\n",
    "\n",
    "    # Save the volume distribution\n",
    "    file_path = os.path.join(PATH_VOL_DISTR, f\"volumes_level_{level}.pkl\")\n",
    "    vols_level.to_pickle(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Pre-generate expert trajectories\n",
    "\n",
    "In the second section, we pre-generate the expert trajectories using the previously downloaded limit order book data. The expert trajectories are generated by simulating the expert-like market making agent's actions in the backtest simulator. These expert trajectories are later used in the adversarial inverse reinforcement learning algorithm to train the discriminator (or reward neural network) for distinguishing the expert trajectories from the adversarial agent's trajectories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Register a custom vectorized environment\n",
    "\n",
    "Firstly, we need to load and register our custom limit order book environment as a custom gym vectorized environment. This is necessary for the [stable-baselines3](https://stable-baselines3.readthedocs.io/en/master/) library to work with the environment. This environment was implemented as a part of the paper and can be found in the [`src/lob`](./src/lob/) module and [`src/rl/environments.py`](./src/rl/environments.py) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "EXCHANGE_NAME = \"BIT.COM\"\n",
    "SYMBOL = \"SOL-USDT\"\n",
    "TICK_SIZE = get_tick_size(EXCHANGE_NAME)  # Tick size of the limit order book\n",
    "LOT_SIZE = get_lot_size(EXCHANGE_NAME)  # Lot size of the limit order book\n",
    "DEPTH = 20  # Depth of the data to load to the limit order book (max 20)\n",
    "EXCHANGE_TRADER_ID = \"Exchange\"\n",
    "MAX_STEPS = 300  # Maximum number of steps in a single episode\n",
    "TS_START = pd.Timestamp(\"2023-09-01 00:00:00\")  # Start of the episode\n",
    "TS_END = pd.Timestamp(\"2023-09-10 23:59:59\")  # End of the episode\n",
    "DETERMINISTIC = False  # Indicates whether to use a deterministic environment\n",
    "WIN = 0  # Window size for the features computation\n",
    "LOGGING = False  # Indicates whether to log events\n",
    "TS_SAVE = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")  # Ts for model saving\n",
    "LATENCY_COMP_PARAMS = {}  # Parameters for the stochastic backtest\n",
    "RNG = np.random.default_rng(seed=SEED)  # Random number generator\n",
    "traders = {}  # Dictionary of traders\n",
    "\n",
    "print(\"Timestamp for saving: \", TS_SAVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for the RL agent\n",
    "rl_trader_id = \"RLMarketMaker\"\n",
    "com_model = BitCommissions(tier=5)\n",
    "volume = 100\n",
    "\n",
    "# Initialize the trader\n",
    "trader = RLMarketMaker(\n",
    "    id=rl_trader_id,\n",
    "    com_model=com_model,\n",
    "    volume=volume,\n",
    ")\n",
    "traders[rl_trader_id] = trader\n",
    "\n",
    "# Write a description of the experiment\n",
    "description = \"RL market maker simulation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for the limit order book environment\n",
    "ID = \"LimitOrderBookGym-v1\"\n",
    "ENTRY_POINT = LimitOrderBookGym\n",
    "KWARGS = {\n",
    "    \"exchange_name\": EXCHANGE_NAME,\n",
    "    \"symbol_name\": SYMBOL,\n",
    "    \"tick_size\": TICK_SIZE,\n",
    "    \"lot_size\": LOT_SIZE,\n",
    "    \"depth\": DEPTH,\n",
    "    \"traders\": traders,\n",
    "    \"max_steps\": MAX_STEPS,\n",
    "    \"ts_start\": TS_START,\n",
    "    \"ts_end\": TS_END,\n",
    "    \"deterministic\": DETERMINISTIC,\n",
    "    \"win\": WIN,\n",
    "    \"path\": PATH,\n",
    "    \"path_vol_distr\": PATH_VOL_DISTR,\n",
    "    \"rl_trader_id\": rl_trader_id,\n",
    "    \"latency_comp_params\": LATENCY_COMP_PARAMS,\n",
    "    \"logging\": LOGGING,\n",
    "    \"ts_save\": TS_SAVE,\n",
    "    \"description\": description,\n",
    "    \"rng\": RNG,\n",
    "}\n",
    "\n",
    "# Register the environment\n",
    "gym.envs.register(\n",
    "    id=ID,\n",
    "    entry_point=ENTRY_POINT,\n",
    "    kwargs=KWARGS,\n",
    "    max_episode_steps=MAX_STEPS,\n",
    ")\n",
    "\n",
    "# Create the environment\n",
    "env = Monitor(gym.make(ID))\n",
    "\n",
    "# Save the saving ts\n",
    "ts_save = env.unwrapped.exchange.ts_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the vectorized environment\n",
    "venv = make_vec_env(\n",
    "    ID,\n",
    "    rng=RNG,\n",
    "    n_envs=1,\n",
    "    post_wrappers=[\n",
    "        lambda env, _: RolloutInfoWrapper(env)\n",
    "    ],  # needed for computing rollouts later\n",
    "    parallel=False,\n",
    ")\n",
    "venv = VecCheckNan(venv, raise_exception=True)  # Check for NaN observations\n",
    "venv.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Generate trajectories with random and expert policies\n",
    "\n",
    "Next, we define an expert policy that will be used as a target for the adversarial agent. We use this expert policy to generate the expert trajectories for the AIRL algorithm. These trajectories are saved to the `data/rollouts` directory and loaded before training to save time in the training process. Alternatively, you can also generate these during the AIRL training process but this will take more time rather than loading and sampling from the pre-generated expert trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for the rollout\n",
    "min_timesteps = None\n",
    "min_episodes = 1\n",
    "\n",
    "# Rollout the environment with a random policy\n",
    "rollouts = rollout.rollout(\n",
    "    None,  # Random policy\n",
    "    venv,\n",
    "    sample_until=rollout.make_sample_until(\n",
    "        min_timesteps=min_timesteps, min_episodes=min_episodes\n",
    "    ),\n",
    "    rng=RNG,\n",
    ")\n",
    "\n",
    "# Print the first rollout\n",
    "if PRINT:\n",
    "    for i in range(len(rollouts[0].obs) - 1):\n",
    "        print(\"Observation: \", rollouts[0].obs[i])\n",
    "        print(\"Action: \", rollouts[0].acts[i])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the random policy\n",
    "random_policy = RandomPolicy(venv.action_space)\n",
    "\n",
    "# Evaluate the random policy\n",
    "reward_random_policy, _ = evaluate_policy(\n",
    "    random_policy, env, 1, return_episode_rewards=True\n",
    ")\n",
    "print(\"Reward of the random policy\")\n",
    "print(\"---------------------------\")\n",
    "print(\"Reward: \", np.mean(reward_random_policy))\n",
    "print(\"Std:    \", np.std(reward_random_policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the expert policy\n",
    "expert = ExpertPolicy()\n",
    "\n",
    "# Set parameters for the rollout\n",
    "min_timesteps = None\n",
    "min_episodes = 1\n",
    "\n",
    "# Rollout the environment with the expert policy\n",
    "rollouts = rollout.rollout(\n",
    "    expert.predict,\n",
    "    venv,\n",
    "    sample_until=rollout.make_sample_until(\n",
    "        min_timesteps=min_timesteps, min_episodes=min_episodes\n",
    "    ),\n",
    "    rng=RNG,\n",
    ")\n",
    "\n",
    "# Print the first rollout\n",
    "if PRINT:\n",
    "    for i in range(len(rollouts[0].obs) - 1):\n",
    "        state, act = rollouts[0].obs[i][0], rollouts[0].acts[i]\n",
    "        print(f\"State 0: {state: .3f} --> Action: {act}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten trajectories into transitions\n",
    "transitions = rollout.flatten_trajectories(rollouts)\n",
    "if PRINT:\n",
    "    print(\"Transitions: \", transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the expert\n",
    "reward_expert_policy, _ = evaluate_policy(\n",
    "    expert, venv, 1, return_episode_rewards=True\n",
    ")\n",
    "print(\"Perfect reward of the expert policy\")\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Reward: \", np.mean(reward_expert_policy))\n",
    "print(\"Std:    \", np.std(reward_expert_policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate or load a presaved sample of expert trajectories.\n",
    "# These rollouts are later used to train the AIRL agent, in particular they are\n",
    "# used for training of the discriminator.\n",
    "rollouts_file = os.path.join(PATH_ROLLOUTS, \"rollouts.pkl\")\n",
    "\n",
    "# If the presaved rollouts file exists, load the rollouts\n",
    "if os.path.exists(rollouts_file):\n",
    "    rollouts = serialize.load(rollouts_file)\n",
    "\n",
    "# Else, generate the rollouts and save them for future use\n",
    "else:\n",
    "    # Set the parameters for the rollout\n",
    "    min_timesteps = 45000 * 3 + 4500\n",
    "    min_episodes = None\n",
    "\n",
    "    # Rollout the environment with the expert policy\n",
    "    print(\"Generating rollouts... (Might take around 15 minutes.)\")\n",
    "    rollouts = rollout.rollout(\n",
    "        expert.predict,\n",
    "        venv,\n",
    "        sample_until=rollout.make_sample_until(\n",
    "            min_timesteps=min_timesteps, min_episodes=min_episodes\n",
    "        ),\n",
    "        rng=RNG,\n",
    "    )\n",
    "\n",
    "    # Ensure the directory exists and save the rollouts\n",
    "    serialize.save(rollouts_file, rollouts)\n",
    "\n",
    "# Print the first rollout\n",
    "if PRINT:\n",
    "    for i in range(len(rollouts[0].obs) - 1):\n",
    "        print(\"Observation: \", rollouts[0].obs[i])\n",
    "        print(\"Action: \", rollouts[0].acts[i])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The saved expert trajectories can be found in the [`data/rollouts`](./data/rollouts) directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Training of the AIRL agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section demonstrates a pipeline for training of the [Adversarial Inverse Reinforcement Learning agent](https://imitation.readthedocs.io/en/latest/algorithms/airl.html) implemented in the [imitation](https://imitation.readthedocs.io/en/latest/index.html) package described in the paper [Imitation: Clean imitation learning implementations - Gleave et al. (2022)](https://arxiv.org/pdf/2211.11972). The goal is to learn the reward function of the expert policy by training of the discriminator network and the agent policy network ([PPO](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html) from the paper [Proximal Policy Optimization Algorithms - Schulman et al. (2017)](https://arxiv.org/abs/1707.06347)). We use the implementation from the [stable-baselines3](https://stable-baselines3.readthedocs.io/en/master/) library described in the paper [Stable-baselines3: Reliable reinforcement learning implementations - Raffin et al. (2021)](https://www.jmlr.org/papers/volume22/20-1364/20-1364.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Initialize the AIRL agent\n",
    "\n",
    "In this subsection, we set hyperparameters for the [PPO algorithm](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html) and the AIRL discriminator ([reward neural network](https://imitation.readthedocs.io/en/latest/main-concepts/reward_networks.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for the PPO algorithm (generator)\n",
    "learning_rate = 0.001  # Learning rate, can be a function of progress\n",
    "batch_size = 60  # Mini batch size for each gradient update\n",
    "n_epochs = 10  # N of epochs when optimizing the surrogate loss\n",
    "\n",
    "gamma = 0.5  # Discount factor, focus on the recent rewards\n",
    "gae_lambda = 0  # Generalized advantage estimation\n",
    "clip_range = 0.1  # Clipping parameter\n",
    "ent_coef = 0.01  # Entropy coefficient for the loss calculation\n",
    "vf_coef = 0.5  # Value function coef. for the loss calculation\n",
    "max_grad_norm = 0.5  # The maximum value for the gradient clipping\n",
    "\n",
    "verbose = 0  # Verbosity level: 0 no output, 1 info, 2 debug\n",
    "normalize_advantage = True  # Whether to normalize or not the advantage\n",
    "\n",
    "clip_range_vf = None  # Clip for the value function\n",
    "use_sde = False  # Use State Dependent Exploration\n",
    "sde_sample_freq = -1  # SDE - noise matrix frequency (-1 = disable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for the (negative) reward net\n",
    "use_state = True  # Whether to use current state in the reward\n",
    "use_action = True  # Whether to use current action in the reward\n",
    "use_next_state = False  # Whether to use next state in the reward\n",
    "use_done = False  # Whether to use done flag in the reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for the AIRL trainer\n",
    "gen_replay_buffer_capacity = None\n",
    "allow_variable_horizon = True\n",
    "\n",
    "disc_opt_kwargs = {\n",
    "    \"lr\": 0.001,\n",
    "}\n",
    "policy_kwargs = {\"use_expln\": True}  # Fixing an issue with NaNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='orange'>**Warning:**</font> Be careful when updating the settings below. When changing them, use the multiples of episode length (otherwise you might run into unexpected issues with variable horizons during training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of timesteps, batch size and number of disc updates\n",
    "\n",
    "# Total number of timesteps in the whole training\n",
    "total_timesteps = 3000 * 600\n",
    "\n",
    "# Generator\n",
    "gen_train_timesteps = 3000  # N steps in the environment per one round\n",
    "n_steps = gen_train_timesteps\n",
    "\n",
    "# Discriminator batches\n",
    "demo_minibatch_size = 60  # N samples in minibatch for one discrim. update\n",
    "demo_batch_size = 300 * 10  # N samples in the batch of expert data (batch)\n",
    "n_disc_updates_per_round = 4  # N discriminator updates per one round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the learner PPO policy (generator)\n",
    "learner = PPO(\n",
    "    env=venv,\n",
    "    policy=MlpPolicy,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    learning_rate=learning_rate,\n",
    "    n_steps=n_steps,\n",
    "    batch_size=batch_size,\n",
    "    n_epochs=n_epochs,\n",
    "    gamma=gamma,\n",
    "    gae_lambda=gae_lambda,\n",
    "    clip_range=clip_range,\n",
    "    clip_range_vf=clip_range_vf,\n",
    "    normalize_advantage=normalize_advantage,\n",
    "    ent_coef=ent_coef,\n",
    "    vf_coef=vf_coef,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    use_sde=use_sde,\n",
    "    sde_sample_freq=sde_sample_freq,\n",
    "    verbose=verbose,\n",
    "    seed=SEED,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the custom reward network (discriminator)\n",
    "reward_net = NegativeRewardNet(\n",
    "    observation_space=venv.observation_space,\n",
    "    action_space=venv.action_space,\n",
    "    use_state=use_state,\n",
    "    use_action=use_action,\n",
    "    use_next_state=use_next_state,\n",
    "    use_done=use_done,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the AIRL trainer\n",
    "airl_trainer = AIRL(\n",
    "    demonstrations=rollouts,\n",
    "    demo_batch_size=demo_batch_size,\n",
    "    demo_minibatch_size=demo_minibatch_size,\n",
    "    n_disc_updates_per_round=n_disc_updates_per_round,\n",
    "    gen_train_timesteps=gen_train_timesteps,\n",
    "    gen_replay_buffer_capacity=gen_replay_buffer_capacity,\n",
    "    venv=venv,\n",
    "    gen_algo=learner,\n",
    "    reward_net=reward_net,\n",
    "    allow_variable_horizon=allow_variable_horizon,\n",
    "    disc_opt_kwargs=disc_opt_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the PPO policy before training\n",
    "venv.seed(SEED)\n",
    "learner_rewards_before_training, _ = evaluate_policy(\n",
    "    learner, venv, 1, return_episode_rewards=True\n",
    ")\n",
    "print(\"Reward of the learner policy before training\")\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"Mean: \", np.mean(learner_rewards_before_training))\n",
    "print(\"Std: \", np.std(learner_rewards_before_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize actions of the PPO policy before training\n",
    "if PRINT:\n",
    "    for _ in range(1):\n",
    "        obs = venv.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            print(obs)\n",
    "            action, _ = learner.predict(obs, deterministic=True)\n",
    "            print(action)\n",
    "            print()\n",
    "            obs, _, done, _ = venv.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Train the AIRL agent\n",
    "\n",
    "Finally, here we train the [adversarial inverse reinforcement learning agent](https://imitation.readthedocs.io/en/latest/algorithms/airl.html). The training process iterates between two steps:\n",
    "1. **Discriminator training** - Train the discriminator (reward neural network) to distinguish between the expert and adversarial agent's trajectories.\n",
    "2. **PPO training** - Train the PPO agent to maximize the reward from the discriminator.\n",
    "\n",
    "The training process is repeated for the specified number of iterations (implied by the total number of steps and number of steps for training of the generator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the AIRL agent\n",
    "airl_trainer.train(total_timesteps=total_timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Save the trained model and training statistics\n",
    "\n",
    "Lastly, save the trained model and the training statistics for further analysis and backtesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "ts = airl_trainer.ts_now\n",
    "stats = airl_trainer.logger._logger.stats\n",
    "save_model(learner, reward_net, stats, PATH_MODELS, ts)\n",
    "print(f\"Saved the model with timestamp: {ts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The saved model, reward neural network and training statistics are saved to the directory defined by the `PATH_MODELS` variable into files denoted by the `ts` timestamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Evaluation of the AIRL agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the timestamp of the model to load\n",
    "# ts = \"2024-01-21_17-12-35\" # seed 1\n",
    "# ts = \"2024-01-22_18-03-01\" # seed 2\n",
    "# ts = \"2024-01-23_19-14-27\" # seed 3\n",
    "# ts = \"2024-01-24_09-40-47\" # seed 4\n",
    "ts = \"2024-01-24_22-39-37\"  # seed 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the new path\n",
    "learner, reward_net, stats = load_model(PATH_MODELS, ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the stats\n",
    "visualize_airl_train_stats(stats, save_fig=SAVE_FIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the policy after training\n",
    "venv.seed(SEED)\n",
    "learner_rewards_after_training, _ = evaluate_policy(\n",
    "    learner, venv, 5, return_episode_rewards=True\n",
    ")\n",
    "print(\"Mean: \", np.mean(learner_rewards_after_training))\n",
    "print(\"Std: \", np.std(learner_rewards_after_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize actions of the policy after training\n",
    "if PRINT:\n",
    "    for _ in range(1):\n",
    "        obs = venv.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = learner.predict(obs, deterministic=True)\n",
    "            print(f\"Obs: {obs[0][0]: .5f} --> Action: {action}\")\n",
    "            obs, _, done, _ = venv.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Visualize the backtest results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "EXCHANGE_NAME = \"BIT.COM\"\n",
    "SYMBOL = \"SOL-USDT\"\n",
    "TICK_SIZE = get_tick_size(EXCHANGE_NAME)  # Tick size of the limit order book\n",
    "LOT_SIZE = get_lot_size(EXCHANGE_NAME)  # Lot size of the limit order book\n",
    "DEPTH = 20  # Depth of the data to load to the limit order book (max 20)\n",
    "EXCHANGE_TRADER_ID = \"Exchange\"\n",
    "MAX_STEPS = None  # Maximum number of steps in an episode\n",
    "TS_START = pd.Timestamp(\"2023-09-11 00:00:00\")  # Start of the episode\n",
    "TS_END = pd.Timestamp(\"2023-09-13 23:59:59\")  # End of the episode\n",
    "WIN = 0  # Window size for the features computation\n",
    "LOGGING = False  # Indicates whether to log events\n",
    "LATENCY_COMP_PARAMS = {\n",
    "    0: {\"prob\": 0.9, \"divisor\": 1},\n",
    "    1: {\"prob\": 0.9, \"divisor\": 1},\n",
    "    2: {\"prob\": 0.9, \"divisor\": 1},\n",
    "    3: {\"prob\": 0.9, \"divisor\": 1},\n",
    "}  # Latency compensation parameters for the stochastic backtest\n",
    "RNG = np.random.default_rng(seed=SEED)  # Random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the limit order book and traders\n",
    "start = time.time()\n",
    "traders = {}\n",
    "\n",
    "# Behavior cloning agent\n",
    "rl_trader_id = \"RLMarketMaker\"\n",
    "com_model = BitCommissions(tier=5)\n",
    "volume = 100\n",
    "trader = RLMarketMaker(\n",
    "    id=rl_trader_id,\n",
    "    com_model=com_model,\n",
    "    volume=volume,\n",
    "    policy=learner.policy,\n",
    ")\n",
    "traders[rl_trader_id] = trader\n",
    "\n",
    "description = \"AIRL agent.\"\n",
    "\n",
    "# Initialize the exchange\n",
    "exchange = Exchange(\n",
    "    exchange_name=EXCHANGE_NAME,\n",
    "    symbol_name=SYMBOL,\n",
    "    tick_size=TICK_SIZE,\n",
    "    lot_size=LOT_SIZE,\n",
    "    depth=DEPTH,\n",
    "    traders=traders,\n",
    "    max_steps=MAX_STEPS,\n",
    "    ts_start=TS_START,\n",
    "    ts_end=TS_END,\n",
    "    win=WIN,\n",
    "    path=PATH,\n",
    "    path_vol_distr=PATH_VOL_DISTR,\n",
    "    rl_trader_id=rl_trader_id,\n",
    "    latency_comp_params=LATENCY_COMP_PARAMS,\n",
    "    logging=LOGGING,\n",
    "    ts_save=TS_SAVE,\n",
    "    description=description,\n",
    "    rng=RNG,\n",
    ")\n",
    "end = round(time.time() - start, 2)\n",
    "print(f\"Time taken for initialization of the exchange: {end} sec.\")\n",
    "\n",
    "# Run the exchange simulation\n",
    "start = time.time()\n",
    "exchange.run()\n",
    "end = round(time.time() - start, 2)\n",
    "print(f\"Time taken for running the exchange: {end} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the backtest results\n",
    "timestamps = exchange.stats[\"ts\"]\n",
    "trader_stats = traders[rl_trader_id].stats\n",
    "initial_cost = 20.5 * volume * 2\n",
    "visualize_backtest(timestamps, trader_stats, initial_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Backtests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Not needed?\n",
    "# # Configure Polars\n",
    "# cfg = pl.Config()\n",
    "# cfg.set_tbl_rows(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pure market makers (with different priorities)\n",
    "\n",
    "In this section I generate the statistics of the pure market making strategy with multiple priorities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TS_START = pd.Timestamp(\"2023-09-01 00:00:00\")  # Start of the episode\n",
    "TS_END = pd.Timestamp(\"2023-09-13 23:59:59\")  # End of the episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "EXCHANGE_NAME = \"BIT.COM\"\n",
    "SYMBOL = \"SOL-USDT\"\n",
    "TICK_SIZE = get_tick_size(EXCHANGE_NAME)  # Tick size of the limit order book\n",
    "LOT_SIZE = get_lot_size(EXCHANGE_NAME)  # Lot size of the limit order book\n",
    "DEPTH = 20  # Depth of the data to load to the limit order book (max 20)\n",
    "EXCHANGE_TRADER_ID = \"Exchange\"\n",
    "MAX_STEPS = None  # Maximum number of steps in an episode\n",
    "WIN = 0  # Window size for the features computation\n",
    "# LOGGING = False # Indicates whether to log events\n",
    "LOGGING = True\n",
    "TS_SAVE = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")  # Ts for model saving\n",
    "RNG = np.random.default_rng(seed=SEED)  # Random number generator\n",
    "\n",
    "# Set the parameters for the stochastic backtest\n",
    "LATENCY_COMP_PARAMS = {\n",
    "    0: {\"prob\": 0.9, \"divisor\": 1},\n",
    "    1: {\"prob\": 0.9, \"divisor\": 1},\n",
    "    2: {\"prob\": 0.9, \"divisor\": 1},\n",
    "    3: {\"prob\": 0.9, \"divisor\": 1},\n",
    "}  # Latency compensation parameters for the stochastic backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters for the automated backtest\n",
    "priorities = [0, 1, 2, 3]\n",
    "volumes = [100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the results dictionary\n",
    "results = {}\n",
    "\n",
    "# Run the backtests\n",
    "for priority in priorities:\n",
    "    for volume in volumes:\n",
    "        # Initialize the limit order book and traders\n",
    "        start = time.time()\n",
    "        traders = {}\n",
    "\n",
    "        # Pure market making strategy\n",
    "        trader_id = f\"PMM_prior_{priority}_vol_{volume}\"\n",
    "        inventory_manage = True\n",
    "        description = (\n",
    "            f\"Pure market maker with priority {priority} and volume {volume}.\"\n",
    "        )\n",
    "\n",
    "        # Set the commission model\n",
    "        if volume == 100:\n",
    "            if priority == 0:\n",
    "                tier = 5\n",
    "            elif priority == 1:\n",
    "                tier = 5\n",
    "            elif priority == 2:\n",
    "                tier = 2\n",
    "            elif priority == 3:\n",
    "                tier = 1\n",
    "        elif volume == 10:\n",
    "            if priority == 0:\n",
    "                tier = 4\n",
    "            elif priority == 1:\n",
    "                tier = 3\n",
    "            elif priority == 2:\n",
    "                tier = 1\n",
    "            elif priority == 3:\n",
    "                tier = 1\n",
    "\n",
    "        com_model = BitCommissions(tier=tier)\n",
    "        trader = PureMarketMaker(\n",
    "            trader_id,\n",
    "            com_model=com_model,\n",
    "            volume=volume,\n",
    "            priority=priority,\n",
    "            inventory_manage=inventory_manage,\n",
    "        )\n",
    "        traders[trader.id] = trader\n",
    "\n",
    "        # Initialize the exchange\n",
    "        exchange = Exchange(\n",
    "            exchange_name=EXCHANGE_NAME,\n",
    "            symbol_name=SYMBOL,\n",
    "            tick_size=TICK_SIZE,\n",
    "            lot_size=LOT_SIZE,\n",
    "            depth=DEPTH,\n",
    "            traders=traders,\n",
    "            max_steps=MAX_STEPS,\n",
    "            ts_start=TS_START,\n",
    "            ts_end=TS_END,\n",
    "            win=WIN,\n",
    "            path=PATH,\n",
    "            path_vol_distr=PATH_VOL_DISTR,\n",
    "            rl_trader_id=\"\",\n",
    "            latency_comp_params=LATENCY_COMP_PARAMS,\n",
    "            logging=LOGGING,\n",
    "            ts_save=TS_SAVE,\n",
    "            description=description,\n",
    "            rng=RNG,\n",
    "        )\n",
    "        end = round(time.time() - start, 2)\n",
    "\n",
    "        # Run the exchange simulation\n",
    "        start = time.time()\n",
    "        exchange.run()\n",
    "        end = round(time.time() - start, 2)\n",
    "\n",
    "        # Save the results\n",
    "        timestamps = exchange.stats[\"ts\"]\n",
    "        trader_stats = traders[trader_id].stats\n",
    "        initial_cost = 20.5 * volume * 2\n",
    "        results[trader_id] = {\n",
    "            \"timestamps\": timestamps,\n",
    "            \"trader_stats\": trader_stats,\n",
    "            \"initial_cost\": initial_cost,\n",
    "        }\n",
    "\n",
    "send_notification(message=\"Backtest finished!\", time=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a pickle file\n",
    "file_path = os.path.join(PATH_AUTOMATED_BACKTESTS, f\"results_{TS_SAVE}.pickle\")\n",
    "with open(file_path, \"wb\") as handle:\n",
    "    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(f\"Results saved to {file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pure market maker (50 seeds)\n",
    "\n",
    "In this section I generate the statistics of the pure market making strategy with multiple priorities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TS_START = pd.Timestamp(\"2023-09-11 00:00:00\")  # Start of the episode\n",
    "TS_END = pd.Timestamp(\"2023-09-13 23:59:59\")  # End of the episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "EXCHANGE_NAME = \"BIT.COM\"\n",
    "SYMBOL = \"SOL-USDT\"\n",
    "TICK_SIZE = get_tick_size(EXCHANGE_NAME)  # Tick size of the limit order book\n",
    "LOT_SIZE = get_lot_size(EXCHANGE_NAME)  # Lot size of the limit order book\n",
    "DEPTH = 20  # Depth of the data to load to the limit order book (max 20)\n",
    "EXCHANGE_TRADER_ID = \"Exchange\"\n",
    "MAX_STEPS = None  # Maximum number of steps in an episode\n",
    "WIN = 0  # Window size for the features computation\n",
    "# LOGGING = False # Indicates whether to log events\n",
    "LOGGING = False\n",
    "TS_SAVE = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")  # Ts for model saving\n",
    "\n",
    "\n",
    "# Set the parameters for the stochastic backtest\n",
    "LATENCY_COMP_PARAMS = {\n",
    "    0: {\"prob\": 0.9, \"divisor\": 1},\n",
    "    1: {\"prob\": 0.9, \"divisor\": 1},\n",
    "    2: {\"prob\": 0.9, \"divisor\": 1},\n",
    "    3: {\"prob\": 0.9, \"divisor\": 1},\n",
    "}  # Latency compensation parameters for the stochastic backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters for the automated backtest\n",
    "priorities = [1]\n",
    "volumes = [100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the results dictionary\n",
    "results = {}\n",
    "\n",
    "# Run the backtests\n",
    "for seed in range(1, 51):\n",
    "    for priority in priorities:\n",
    "        for volume in volumes:\n",
    "            RNG = np.random.default_rng(seed=seed)\n",
    "\n",
    "            # Initialize the limit order book and traders\n",
    "            start = time.time()\n",
    "            traders = {}\n",
    "\n",
    "            # Pure market making strategy\n",
    "            trader_id = f\"PMM_prior_{priority}_vol_{volume}_{seed}\"\n",
    "            inventory_manage = True\n",
    "            description = (\n",
    "                f\"Pure market maker with priority {priority} and volume \",\n",
    "                f\"{volume}.\",\n",
    "            )\n",
    "\n",
    "            # Set the commission model\n",
    "            if volume == 100:\n",
    "                if priority == 0:\n",
    "                    tier = 5\n",
    "                elif priority == 1:\n",
    "                    tier = 5\n",
    "                elif priority == 2:\n",
    "                    tier = 2\n",
    "                elif priority == 3:\n",
    "                    tier = 1\n",
    "            elif volume == 10:\n",
    "                if priority == 0:\n",
    "                    tier = 4\n",
    "                elif priority == 1:\n",
    "                    tier = 3\n",
    "                elif priority == 2:\n",
    "                    tier = 1\n",
    "                elif priority == 3:\n",
    "                    tier = 1\n",
    "\n",
    "            com_model = BitCommissions(tier=tier)\n",
    "            trader = PureMarketMaker(\n",
    "                trader_id,\n",
    "                com_model=com_model,\n",
    "                volume=volume,\n",
    "                priority=priority,\n",
    "                inventory_manage=inventory_manage,\n",
    "            )\n",
    "            traders[trader.id] = trader\n",
    "\n",
    "            # Initialize the exchange\n",
    "            exchange = Exchange(\n",
    "                exchange_name=EXCHANGE_NAME,\n",
    "                symbol_name=SYMBOL,\n",
    "                tick_size=TICK_SIZE,\n",
    "                lot_size=LOT_SIZE,\n",
    "                depth=DEPTH,\n",
    "                traders=traders,\n",
    "                max_steps=MAX_STEPS,\n",
    "                ts_start=TS_START,\n",
    "                ts_end=TS_END,\n",
    "                win=WIN,\n",
    "                path=PATH,\n",
    "                path_vol_distr=PATH_VOL_DISTR,\n",
    "                rl_trader_id=\"\",\n",
    "                latency_comp_params=LATENCY_COMP_PARAMS,\n",
    "                logging=LOGGING,\n",
    "                ts_save=TS_SAVE,\n",
    "                description=description,\n",
    "                rng=RNG,\n",
    "            )\n",
    "            end = round(time.time() - start, 2)\n",
    "\n",
    "            # Run the exchange simulation\n",
    "            start = time.time()\n",
    "            exchange.run()\n",
    "            end = round(time.time() - start, 2)\n",
    "\n",
    "            # Save the results\n",
    "            timestamps = exchange.stats[\"ts\"]\n",
    "            trader_stats = traders[trader_id].stats\n",
    "            initial_cost = 20.5 * volume * 2\n",
    "            results[trader_id] = {\n",
    "                \"timestamps\": timestamps,\n",
    "                \"trader_stats\": trader_stats,\n",
    "                \"initial_cost\": initial_cost,\n",
    "            }\n",
    "\n",
    "send_notification(message=\"Backtest finished!\", time=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a pickle file\n",
    "file_path = os.path.join(PATH_AUTOMATED_BACKTESTS, f\"results_{TS_SAVE}.pickle\")\n",
    "with open(file_path, \"wb\") as handle:\n",
    "    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(f\"Results saved to {file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIRL market maker (50 seeds)\n",
    "\n",
    "In this section I generate the statistics of the AIRL market making strategy with multiple priorities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TS_START = pd.Timestamp(\"2023-09-11 00:00:00\")  # Start of the episode\n",
    "TS_END = pd.Timestamp(\"2023-09-13 23:59:59\")  # End of the episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "EXCHANGE_NAME = \"BIT.COM\"\n",
    "SYMBOL = \"SOL-USDT\"\n",
    "TICK_SIZE = get_tick_size(EXCHANGE_NAME)  # Tick size of the limit order book\n",
    "LOT_SIZE = get_lot_size(EXCHANGE_NAME)  # Lot size of the limit order book\n",
    "DEPTH = 20  # Depth of the data to load to the limit order book (max 20)\n",
    "EXCHANGE_TRADER_ID = \"Exchange\"\n",
    "MAX_STEPS = None  # Maximum number of steps in an episode\n",
    "WIN = 0  # Window size for the features computation\n",
    "# LOGGING = False # Indicates whether to log events\n",
    "LOGGING = False\n",
    "TS_SAVE = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")  # Ts for model saving\n",
    "\n",
    "# Set the parameters for the stochastic backtest\n",
    "LATENCY_COMP_PARAMS = {\n",
    "    0: {\"prob\": 0.9, \"divisor\": 1},\n",
    "    1: {\"prob\": 0.9, \"divisor\": 1},\n",
    "    2: {\"prob\": 0.9, \"divisor\": 1},\n",
    "    3: {\"prob\": 0.9, \"divisor\": 1},\n",
    "}  # Latency compensation parameters for stochastic backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters for the automated backtest\n",
    "priorities = [1]\n",
    "volumes = [100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the results dictionary\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the timestamp of the model to load\n",
    "# ts = \"2024-01-21_17-12-35\" # seed 1\n",
    "# ts = \"2024-01-22_18-03-01\" # seed 2\n",
    "# ts = \"2024-01-23_19-14-27\" # seed 3\n",
    "# ts = \"2024-01-24_09-40-47\" # seed 4\n",
    "# ts = \"2024-01-24_22-39-37\" # seed 5\n",
    "ts = \"2024-01-24_22-39-37_best_9_297.5\"  # seed 5 (best model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model)\n",
    "learner, reward_net, stats = load_model(PATH_MODELS, ts)\n",
    "print(f\"Loaded model for timestamp: {ts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(1, 51):\n",
    "    for priority in priorities:\n",
    "        for volume in volumes:\n",
    "            RNG = np.random.default_rng(seed=seed)\n",
    "\n",
    "            # Initialize the limit order book and traders\n",
    "            start = time.time()\n",
    "            traders = {}\n",
    "\n",
    "            # # Pure market making strategy\n",
    "            trader_id = f\"RL_prior_{priority}_vol_{volume}_{seed}\"\n",
    "            # inventory_manage = True\n",
    "            description = (\n",
    "                f\"RL market maker with priority {priority} and volume {volume}.\"\n",
    "            )\n",
    "\n",
    "            # Set the commission model\n",
    "            if volume == 100:\n",
    "                if priority == 0:\n",
    "                    tier = 5\n",
    "                elif priority == 1:\n",
    "                    tier = 5\n",
    "                elif priority == 2:\n",
    "                    tier = 2\n",
    "                elif priority == 3:\n",
    "                    tier = 1\n",
    "            elif volume == 10:\n",
    "                if priority == 0:\n",
    "                    tier = 4\n",
    "                elif priority == 1:\n",
    "                    tier = 3\n",
    "                elif priority == 2:\n",
    "                    tier = 1\n",
    "                elif priority == 3:\n",
    "                    tier = 1\n",
    "\n",
    "            com_model = BitCommissions(tier=tier)\n",
    "            trader = RLMarketMaker(\n",
    "                id=trader_id,\n",
    "                com_model=com_model,\n",
    "                volume=volume,\n",
    "                policy=learner.policy,\n",
    "            )\n",
    "            traders[trader.id] = trader\n",
    "\n",
    "            # Initialize the exchange\n",
    "            exchange = Exchange(\n",
    "                exchange_name=EXCHANGE_NAME,\n",
    "                symbol_name=SYMBOL,\n",
    "                tick_size=TICK_SIZE,\n",
    "                lot_size=LOT_SIZE,\n",
    "                depth=DEPTH,\n",
    "                traders=traders,\n",
    "                max_steps=MAX_STEPS,\n",
    "                ts_start=TS_START,\n",
    "                ts_end=TS_END,\n",
    "                win=WIN,\n",
    "                path=PATH,\n",
    "                path_vol_distr=PATH_VOL_DISTR,\n",
    "                rl_trader_id=trader_id,\n",
    "                latency_comp_params=LATENCY_COMP_PARAMS,\n",
    "                logging=LOGGING,\n",
    "                ts_save=TS_SAVE,\n",
    "                description=description,\n",
    "                rng=RNG,\n",
    "            )\n",
    "            end = round(time.time() - start, 2)\n",
    "\n",
    "            # Run the exchange simulation\n",
    "            start = time.time()\n",
    "            exchange.run()\n",
    "            end = round(time.time() - start, 2)\n",
    "\n",
    "            # Save the results\n",
    "            timestamps = exchange.stats[\"ts\"]\n",
    "            trader_stats = traders[trader_id].stats\n",
    "            initial_cost = 20.5 * volume * 2\n",
    "            results[trader_id] = {\n",
    "                \"timestamps\": timestamps,\n",
    "                \"trader_stats\": trader_stats,\n",
    "                \"initial_cost\": initial_cost,\n",
    "            }\n",
    "\n",
    "send_notification(message=\"Backtest finished!\", time=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a pickle file\n",
    "file_path = os.path.join(PATH_AUTOMATED_BACKTESTS, f\"results_{TS_SAVE}.pickle\")\n",
    "with open(file_path, \"wb\") as handle:\n",
    "    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(f\"Results saved to {file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6: Backtest visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize backtests for PMM with priority 1 and 100 volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/automated_backtests/results_2024-02-25_10-58-39.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results from a pickle file\n",
    "with open(file_path, \"rb\") as handle:\n",
    "    results = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PMM priority 1 volume 100\n",
    "\n",
    "# Load the results\n",
    "ts = results[\"PMM_prior_1_vol_100\"][\"timestamps\"]\n",
    "trader_stats = results[\"PMM_prior_1_vol_100\"][\"trader_stats\"]\n",
    "initial_cost = results[\"PMM_prior_1_vol_100\"][\"initial_cost\"]\n",
    "\n",
    "# Plot the results\n",
    "# ----------------------------------------------------------------------------\n",
    "# PLOT - Adjusted PnL\n",
    "path = os.path.join(PATH_FIGURES, \"PMM_pnl_100.pdf\") if SAVE_FIG else None\n",
    "make_plot(\n",
    "    x=ts,\n",
    "    y=trader_stats[\"adj_pnl\"],\n",
    "    xlabel=\"Time\",\n",
    "    ylabel=\"P&L (USDT)\",\n",
    "    save_path=path,\n",
    ")\n",
    "print(f\"Final P&L: {trader_stats['adj_pnl'][-1]}\")\n",
    "\n",
    "# PLOT - Returns\n",
    "equity = pd.Series(np.array(trader_stats[\"adj_pnl\"]) + initial_cost)\n",
    "returns = equity.pct_change() * 100\n",
    "path = os.path.join(PATH_FIGURES, \"PMM_returns_100.pdf\") if SAVE_FIG else None\n",
    "make_plot(\n",
    "    x=ts,\n",
    "    y=returns,\n",
    "    xlabel=\"Time\",\n",
    "    ylabel=\"Returns (%)\",\n",
    "    save_path=path,\n",
    ")\n",
    "print(\"Returns stats\")\n",
    "print(returns.describe())\n",
    "\n",
    "# PLOT - Drawdowns\n",
    "dd = drawdowns(equity)\n",
    "path = os.path.join(PATH_FIGURES, \"PMM_drawdowns_100.pdf\") if SAVE_FIG else None\n",
    "make_drawdown_plot(\n",
    "    x=ts,\n",
    "    y=dd,\n",
    "    xlabel=\"Time\",\n",
    "    ylabel=\"Drawdown (%)\",\n",
    "    save_path=path,\n",
    ")\n",
    "print(\"Drawdown stats\")\n",
    "print(dd.describe())\n",
    "\n",
    "# PLOT - Inventory\n",
    "path = os.path.join(PATH_FIGURES, \"PMM_inventory_100.pdf\") if SAVE_FIG else None\n",
    "make_plot(\n",
    "    x=ts,\n",
    "    y=trader_stats[\"inventory\"],\n",
    "    xlabel=\"Time\",\n",
    "    ylabel=\"Inventory (SOL)\",\n",
    "    color=\"darkorange\",\n",
    "    save_path=path,\n",
    ")\n",
    "print(\"Inventory stats\")\n",
    "print(pd.Series(trader_stats[\"inventory\"]).describe())\n",
    "\n",
    "# PLOT - Total traded volume\n",
    "path = os.path.join(PATH_FIGURES, \"PMM_volume_100.pdf\") if SAVE_FIG else None\n",
    "make_plot(\n",
    "    x=ts,\n",
    "    y=trader_stats[\"total_volume\"],\n",
    "    xlabel=\"Time\",\n",
    "    ylabel=\"Traded volume (USDT)\",\n",
    "    ylim=(-40000, 840000),\n",
    "    color=\"darkorange\",\n",
    "    save_path=path,\n",
    ")\n",
    "print(\"Total volume: \", trader_stats[\"total_volume\"][-1])\n",
    "\n",
    "# PLOT - Transaction costs\n",
    "path = os.path.join(PATH_FIGURES, \"PMM_fees_100.pdf\") if SAVE_FIG else None\n",
    "make_plot(\n",
    "    x=ts,\n",
    "    y=trader_stats[\"cum_costs\"],\n",
    "    xlabel=\"Time\",\n",
    "    ylabel=\"Transaction fees (USDT)\",\n",
    "    ylim=(-20, 420),\n",
    "    color=\"red\",\n",
    "    save_path=path,\n",
    ")\n",
    "print(\"Total fees: \", trader_stats[\"cum_costs\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of returns\n",
    "fig = plt.figure(figsize=FIG_SIZE)\n",
    "plt.hist(returns, bins=50, alpha=1, log=True)\n",
    "# Add kernel density estimate\n",
    "plt.xlabel(\"Returns (%)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "if SAVE_FIG:\n",
    "    plt.savefig(os.path.join(PATH_FIGURES, \"PMM_returns_hist_100.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of PMM strategies with volume 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/automated_backtests/results_2024-02-25_10-58-39.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results from a pickle file\n",
    "with open(file_path, \"rb\") as handle:\n",
    "    results = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PnL plot\n",
    "\n",
    "# Set parameters\n",
    "xlabel = \"Time\"\n",
    "ylabel = \"P&L (USDT)\"\n",
    "\n",
    "plt.figure(figsize=FIG_SIZE)\n",
    "i = 0\n",
    "for key, value in results.items():\n",
    "    x = value[\"timestamps\"]\n",
    "    y = value[\"trader_stats\"][\"adj_pnl\"]\n",
    "    label = f\"PMM (priority {i})\"\n",
    "    plt.plot(x, y, label=label)\n",
    "    print(f\"{key} - {y[-1]:.2f}\")\n",
    "    i += 1\n",
    "\n",
    "# plt.plot(x, y, color=color)\n",
    "plt.xlabel(xlabel)\n",
    "plt.ylabel(ylabel)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "if SAVE_FIG:\n",
    "    plt.savefig(os.path.join(PATH_FIGURES, \"PMM_comparison_pnl_100.pdf\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "xlabel = \"Time\"\n",
    "ylabel = \"Traded volume (USDT)\"\n",
    "\n",
    "plt.figure(figsize=FIG_SIZE)\n",
    "i = 0\n",
    "for key, value in results.items():\n",
    "    x = value[\"timestamps\"]\n",
    "    y = value[\"trader_stats\"][\"total_volume\"]\n",
    "    label = f\"PMM (priority {i})\"\n",
    "    plt.plot(x, y, label=label)\n",
    "    print(f\"{key} - {y[-1]:.2f}\")\n",
    "    i += 1\n",
    "\n",
    "plt.xlabel(xlabel)\n",
    "plt.ylabel(ylabel)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "if SAVE_FIG:\n",
    "    plt.savefig(os.path.join(PATH_FIGURES, \"PMM_comparison_volume_100.pdf\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute hitting probability\n",
    "for key, value in results.items():\n",
    "    trades = np.array(value[\"trader_stats\"][\"trade_count\"])\n",
    "    hits = np.where(trades > 0, 1, 0)\n",
    "\n",
    "    print(f\"{key} - {np.mean(hits)*100:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "\n",
    "# Spread histogram\n",
    "fig = plt.figure(figsize=FIG_SIZE)\n",
    "for key, value in results.items():\n",
    "    spread = np.array(value[\"trader_stats\"][\"quoted_ask_price\"]) - np.array(\n",
    "        value[\"trader_stats\"][\"quoted_bid_price\"]\n",
    "    )\n",
    "    plt.hist(\n",
    "        spread, bins=50, alpha=0.75, log=False, label=f\"PMM (priority {index})\"\n",
    "    )\n",
    "    mean = np.mean(spread)\n",
    "    plt.vlines(mean, 0, 50000, color=f\"C{index}\", linestyle=\"--\")\n",
    "    print(f\"{key} - mean: {mean:.4f}\")\n",
    "    index += 1\n",
    "# Add kernel density estimate\n",
    "plt.xlabel(\"Spread (USDT)\")\n",
    "plt.ylabel(\"Count\")\n",
    "# Cut off the outliers\n",
    "plt.xlim(0, 0.24)\n",
    "plt.xticks(np.arange(0, 0.24, 0.01), rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "if SAVE_FIG:\n",
    "    plt.savefig(os.path.join(PATH_FIGURES, \"PMM_comparison_spread_100.pdf\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of PMM strategies with volume 100 (different SEEDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/automated_backtests/results_2024-02-25_15-16-39.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results from a pickle file\n",
    "with open(file_path, \"rb\") as handle:\n",
    "    results_pmm = pickle.load(handle)\n",
    "\n",
    "results_pmm.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PnL plot\n",
    "\n",
    "# Set parameters\n",
    "xlabel = \"Time\"\n",
    "ylabel = \"P&L (USDT)\"\n",
    "avg = []\n",
    "\n",
    "plt.figure(figsize=FIG_SIZE)\n",
    "i = 0\n",
    "# for key, value in results_pmm.items():\n",
    "for value in results_pmm.values():\n",
    "    x = value[\"timestamps\"]\n",
    "    y = value[\"trader_stats\"][\"adj_pnl\"]\n",
    "    label = f\"PMM (priority {i})\"\n",
    "    plt.plot(x, y, label=label)\n",
    "    # print(f\"{key} - {y[-1]:.2f}\")\n",
    "    i += 1\n",
    "    avg.append(value[\"trader_stats\"][\"adj_pnl\"][-1])\n",
    "\n",
    "# plt.plot(x, y, color=color)\n",
    "plt.xlabel(xlabel)\n",
    "plt.ylabel(ylabel)\n",
    "plt.ylim(-60, 210)\n",
    "# plt.legend()\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "plt.tight_layout()\n",
    "if SAVE_FIG:\n",
    "    plt.savefig(os.path.join(PATH_FIGURES, \"PMM_seeds_comparison_pnl.pdf\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean pnl: {np.mean(avg):.2f}\")\n",
    "print(f\"Std pnl: {np.std(avg):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "xlabel = \"Time\"\n",
    "ylabel = \"Traded volume (USDT)\"\n",
    "avg = []\n",
    "\n",
    "plt.figure(figsize=FIG_SIZE)\n",
    "i = 0\n",
    "# for key, value in results_pmm.items():\n",
    "for value in results_pmm.values():\n",
    "    x = value[\"timestamps\"]\n",
    "    y = value[\"trader_stats\"][\"total_volume\"]\n",
    "    label = f\"PMM (priority {i})\"\n",
    "    plt.plot(x, y, label=label)\n",
    "    # print(f\"{key} - {y[-1]:.2f}\")\n",
    "    avg.append(value[\"trader_stats\"][\"total_volume\"][-1])\n",
    "    i += 1\n",
    "\n",
    "plt.xlabel(xlabel)\n",
    "plt.ylabel(ylabel)\n",
    "# plt.legend()\n",
    "plt.ylim(-10000, 230000)\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "plt.tight_layout()\n",
    "if SAVE_FIG:\n",
    "    plt.savefig(os.path.join(PATH_FIGURES, \"PMM_seeds_comparison_volume.pdf\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean volume: {np.mean(avg):.2f}\")\n",
    "print(f\"Std volume: {np.std(avg):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "xlabel = \"Time\"\n",
    "ylabel = \"Transaction fees (USDT)\"\n",
    "avg = []\n",
    "\n",
    "plt.figure(figsize=FIG_SIZE)\n",
    "i = 0\n",
    "# for key, value in results_pmm.items():\n",
    "for value in results_pmm.values():\n",
    "    x = value[\"timestamps\"]\n",
    "    y = value[\"trader_stats\"][\"cum_costs\"]\n",
    "    label = f\"PMM (priority {i})\"\n",
    "    plt.plot(x, y, label=label)\n",
    "    # print(f\"{key} - {y[-1]:.2f}\")\n",
    "    avg.append(value[\"trader_stats\"][\"cum_costs\"][-1])\n",
    "    i += 1\n",
    "\n",
    "plt.xlabel(xlabel)\n",
    "plt.ylabel(ylabel)\n",
    "# plt.legend()\n",
    "plt.ylim(-5, 115)\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "plt.tight_layout()\n",
    "if SAVE_FIG:\n",
    "    plt.savefig(os.path.join(PATH_FIGURES, \"PMM_seeds_comparison_fees.pdf\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean fees: {np.mean(avg):.2f}\")\n",
    "print(f\"Std fees: {np.std(avg):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = []\n",
    "\n",
    "# Compute hitting probability\n",
    "for value in results_pmm.values():\n",
    "    trades = np.array(value[\"trader_stats\"][\"trade_count\"])\n",
    "    hits = np.where(trades > 0, 1, 0)\n",
    "    avg.append(np.mean(hits) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean hitting probability: {np.mean(avg):.2f}%\")\n",
    "print(f\"Std hitting probability: {np.std(avg):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pmm.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of AIRL strategies with volume 100 (different SEEDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/automated_backtests/results_2024-02-25_21-22-46.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results from a pickle file\n",
    "with open(file_path, \"rb\") as handle:\n",
    "    results_airl = pickle.load(handle)\n",
    "\n",
    "results_airl.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PnL plot\n",
    "\n",
    "# Set parameters\n",
    "xlabel = \"Time\"\n",
    "ylabel = \"P&L (USDT)\"\n",
    "avg = []\n",
    "\n",
    "plt.figure(figsize=FIG_SIZE)\n",
    "i = 0\n",
    "# for key, value in results_airl.items():\n",
    "for value in results_airl.values():\n",
    "    x = value[\"timestamps\"]\n",
    "    y = value[\"trader_stats\"][\"adj_pnl\"]\n",
    "    label = f\"PMM (priority {i})\"\n",
    "    plt.plot(x, y, label=label)\n",
    "    # print(f\"{key} - {y[-1]:.2f}\")\n",
    "    i += 1\n",
    "    avg.append(value[\"trader_stats\"][\"adj_pnl\"][-1])\n",
    "\n",
    "# plt.plot(x, y, color=color)\n",
    "plt.xlabel(xlabel)\n",
    "plt.ylabel(ylabel)\n",
    "plt.ylim(-60, 210)\n",
    "# plt.legend()\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "plt.tight_layout()\n",
    "if SAVE_FIG:\n",
    "    plt.savefig(os.path.join(PATH_FIGURES, \"AIRL_seeds_comparison_pnl.pdf\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean pnl: {np.mean(avg):.2f}\")\n",
    "print(f\"Std pnl: {np.std(avg):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "xlabel = \"Time\"\n",
    "ylabel = \"Traded volume (USDT)\"\n",
    "avg = []\n",
    "\n",
    "plt.figure(figsize=FIG_SIZE)\n",
    "i = 0\n",
    "# for key, value in results_airl.items():\n",
    "for value in results_airl.values():\n",
    "    x = value[\"timestamps\"]\n",
    "    y = value[\"trader_stats\"][\"total_volume\"]\n",
    "    label = f\"PMM (priority {i})\"\n",
    "    plt.plot(x, y, label=label)\n",
    "    # print(f\"{key} - {y[-1]:.2f}\")\n",
    "    avg.append(value[\"trader_stats\"][\"total_volume\"][-1])\n",
    "    i += 1\n",
    "\n",
    "plt.xlabel(xlabel)\n",
    "plt.ylabel(ylabel)\n",
    "# plt.legend()\n",
    "plt.ylim(-10000, 230000)\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "plt.tight_layout()\n",
    "if SAVE_FIG:\n",
    "    plt.savefig(os.path.join(PATH_FIGURES, \"AIRL_seeds_comparison_volume.pdf\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean volume: {np.mean(avg):.2f}\")\n",
    "print(f\"Std volume: {np.std(avg):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "xlabel = \"Time\"\n",
    "ylabel = \"Transaction fees (USDT)\"\n",
    "avg = []\n",
    "\n",
    "plt.figure(figsize=FIG_SIZE)\n",
    "i = 0\n",
    "# for key, value in results_airl.items():\n",
    "for value in results_airl.values():\n",
    "    x = value[\"timestamps\"]\n",
    "    y = value[\"trader_stats\"][\"cum_costs\"]\n",
    "    label = f\"PMM (priority {i})\"\n",
    "    plt.plot(x, y, label=label)\n",
    "    # print(f\"{key} - {y[-1]:.2f}\")\n",
    "    avg.append(value[\"trader_stats\"][\"cum_costs\"][-1])\n",
    "    i += 1\n",
    "\n",
    "# Increase the font size\n",
    "# plt.xlabel(xlabel, fontsize=12)\n",
    "\n",
    "plt.xlabel(xlabel)\n",
    "plt.ylabel(ylabel)\n",
    "# plt.legend()\n",
    "plt.ylim(-5, 115)\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "plt.tight_layout()\n",
    "if SAVE_FIG:\n",
    "    plt.savefig(os.path.join(PATH_FIGURES, \"AIRL_seeds_comparison_fees.pdf\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean fees: {np.mean(avg):.2f}\")\n",
    "print(f\"Std fees: {np.std(avg):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = []\n",
    "\n",
    "# Compute hitting probability\n",
    "for value in results_airl.values():\n",
    "    trades = np.array(value[\"trader_stats\"][\"trade_count\"])\n",
    "    hits = np.where(trades > 0, 1, 0)\n",
    "    avg.append(np.mean(hits) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean hitting probability: {np.mean(avg):.2f}%\")\n",
    "print(f\"Std hitting probability: {np.std(avg):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seed comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the seed for visualization\n",
    "SEED_VISUAL = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PMM\n",
    "\n",
    "# Load the results\n",
    "ts = results_pmm[f\"PMM_prior_1_vol_100_{SEED_VISUAL}\"][\"timestamps\"]\n",
    "trader_stats = results_pmm[f\"PMM_prior_1_vol_100_{SEED_VISUAL}\"][\"trader_stats\"]\n",
    "initial_cost = results_pmm[f\"PMM_prior_1_vol_100_{SEED_VISUAL}\"][\"initial_cost\"]\n",
    "\n",
    "# Plot the results\n",
    "# ----------------------------------------------------------------------------\n",
    "# PLOT - PnL\n",
    "plt.figure(figsize=FIG_SIZE)\n",
    "plt.plot(ts, trader_stats[\"adj_pnl\"])\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"P&L (USDT)\")\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "plt.tight_layout()\n",
    "if SAVE_FIG:\n",
    "    plt.savefig(os.path.join(PATH_FIGURES, \"PMM_seeds_pnl.pdf\"))\n",
    "plt.show()\n",
    "print(f\"Final P&L: {trader_stats['adj_pnl'][-1]}\")\n",
    "\n",
    "# PLOT - Returns\n",
    "equity = pd.Series(np.array(trader_stats[\"adj_pnl\"]) + initial_cost)\n",
    "returns = equity.pct_change() * 100\n",
    "plt.figure(figsize=FIG_SIZE)\n",
    "plt.plot(ts, returns)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Returns (%)\")\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "plt.tight_layout()\n",
    "if SAVE_FIG:\n",
    "    plt.savefig(os.path.join(PATH_FIGURES, \"PMM_seeds_returns.pdf\"))\n",
    "plt.show()\n",
    "print(\"Returns stats\")\n",
    "print(returns.describe())\n",
    "\n",
    "# PLOT - Drawdowns\n",
    "dd = drawdowns(equity)\n",
    "plt.figure(figsize=FIG_SIZE)\n",
    "plt.fill_between(ts, dd, 0, color=\"red\", alpha=0.3)\n",
    "plt.plot(ts, dd, color=\"red\", alpha=0.5)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Drawdown (%)\")\n",
    "plt.ylim(-0.85, 0.05)\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "plt.tight_layout()\n",
    "if SAVE_FIG:\n",
    "    plt.savefig(os.path.join(PATH_FIGURES, \"PMM_seeds_drawdowns.pdf\"))\n",
    "plt.show()\n",
    "print(\"Drawdown stats\")\n",
    "print(dd.describe())\n",
    "\n",
    "# PLOT - Inventory\n",
    "plt.figure(figsize=FIG_SIZE)\n",
    "plt.plot(\n",
    "    ts,\n",
    "    trader_stats[\"inventory\"],\n",
    "    color=\"darkorange\",\n",
    ")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Inventory (SOL)\")\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "plt.tight_layout()\n",
    "if SAVE_FIG:\n",
    "    plt.savefig(os.path.join(PATH_FIGURES, \"PMM_seeds_inventory.pdf\"))\n",
    "plt.show()\n",
    "print(\"Inventory stats\")\n",
    "print(pd.Series(trader_stats[\"inventory\"]).describe())\n",
    "\n",
    "# PLOT - Total traded volume\n",
    "plt.figure(figsize=FIG_SIZE)\n",
    "plt.plot(\n",
    "    ts,\n",
    "    trader_stats[\"total_volume\"],\n",
    "    color=\"darkorange\",\n",
    ")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Traded volume (USDT)\")\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "plt.tight_layout()\n",
    "if SAVE_FIG:\n",
    "    plt.savefig(os.path.join(PATH_FIGURES, \"PMM_seeds_volume.pdf\"))\n",
    "plt.show()\n",
    "print(\"Total volume: \", trader_stats[\"total_volume\"][-1])\n",
    "\n",
    "# PLOT - Transaction costs\n",
    "plt.figure(figsize=FIG_SIZE)\n",
    "plt.plot(\n",
    "    ts,\n",
    "    trader_stats[\"cum_costs\"],\n",
    "    color=\"red\",\n",
    ")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Transaction fees (USDT)\")\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "plt.tight_layout()\n",
    "if SAVE_FIG:\n",
    "    plt.savefig(os.path.join(PATH_FIGURES, \"PMM_seeds_fees.pdf\"))\n",
    "plt.show()\n",
    "print(\"Total fees: \", trader_stats[\"cum_costs\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### AIRL\n",
    "\n",
    "# Load the results\n",
    "airl_ts = results_airl[f\"RL_prior_1_vol_100_{SEED_VISUAL}\"][\"timestamps\"]\n",
    "airl_trader_stats = results_airl[f\"RL_prior_1_vol_100_{SEED_VISUAL}\"][\n",
    "    \"trader_stats\"\n",
    "]\n",
    "airl_initial_cost = results_airl[f\"RL_prior_1_vol_100_{SEED_VISUAL}\"][\n",
    "    \"initial_cost\"\n",
    "]\n",
    "\n",
    "# Plot the results\n",
    "# ----------------------------------------------------------------------------\n",
    "# PLOT - PnL\n",
    "plt.figure(figsize=FIG_SIZE)\n",
    "plt.plot(airl_ts, airl_trader_stats[\"adj_pnl\"], label=\"AIRL\")\n",
    "plt.plot(ts, trader_stats[\"adj_pnl\"], label=\"Expert policy\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"P&L (USDT)\")\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "if SAVE_FIG:\n",
    "    plt.savefig(os.path.join(PATH_FIGURES, \"AIRL_seeds_pnl.pdf\"))\n",
    "plt.show()\n",
    "print(f\"Final P&L: {airl_trader_stats['adj_pnl'][-1]}\")\n",
    "\n",
    "# PLOT - Returns\n",
    "equity = pd.Series(np.array(airl_trader_stats[\"adj_pnl\"]) + initial_cost)\n",
    "airl_returns = equity.pct_change() * 100\n",
    "plt.figure(figsize=FIG_SIZE)\n",
    "plt.plot(airl_ts, airl_returns)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Returns (%)\")\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "plt.tight_layout()\n",
    "if SAVE_FIG:\n",
    "    plt.savefig(os.path.join(PATH_FIGURES, \"AIRL_seeds_returns.pdf\"))\n",
    "plt.show()\n",
    "print(\"Returns stats\")\n",
    "print(returns.describe())\n",
    "\n",
    "# PLOT - Drawdowns\n",
    "dd = drawdowns(equity)\n",
    "plt.figure(figsize=FIG_SIZE)\n",
    "plt.fill_between(ts, dd, 0, color=\"red\", alpha=0.3)\n",
    "plt.plot(airl_ts, dd, color=\"red\", alpha=0.5)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Drawdown (%)\")\n",
    "plt.ylim(-0.85, 0.05)\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "plt.tight_layout()\n",
    "if SAVE_FIG:\n",
    "    plt.savefig(os.path.join(PATH_FIGURES, \"AIRL_seeds_drawdowns.pdf\"))\n",
    "plt.show()\n",
    "print(\"Drawdown stats\")\n",
    "print(dd.describe())\n",
    "\n",
    "# PLOT - Inventory\n",
    "plt.figure(figsize=FIG_SIZE)\n",
    "plt.plot(\n",
    "    airl_ts,\n",
    "    airl_trader_stats[\"inventory\"],\n",
    "    color=\"darkorange\",\n",
    ")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Inventory (SOL)\")\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "plt.tight_layout()\n",
    "if SAVE_FIG:\n",
    "    plt.savefig(os.path.join(PATH_FIGURES, \"AIRL_seeds_inventory.pdf\"))\n",
    "plt.show()\n",
    "print(\"Inventory stats\")\n",
    "print(pd.Series(airl_trader_stats[\"inventory\"]).describe())\n",
    "\n",
    "# PLOT - Total traded volume\n",
    "plt.figure(figsize=FIG_SIZE)\n",
    "plt.plot(airl_ts, airl_trader_stats[\"total_volume\"], label=\"AIRL\")\n",
    "plt.plot(ts, trader_stats[\"total_volume\"], label=\"Expert policy\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Traded volume (USDT)\")\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "if SAVE_FIG:\n",
    "    plt.savefig(os.path.join(PATH_FIGURES, \"AIRL_seeds_volume.pdf\"))\n",
    "plt.show()\n",
    "print(\"Total volume: \", airl_trader_stats[\"total_volume\"][-1])\n",
    "\n",
    "# PLOT - Transaction costs\n",
    "plt.figure(figsize=FIG_SIZE)\n",
    "plt.plot(airl_ts, airl_trader_stats[\"cum_costs\"], label=\"AIRL\")\n",
    "plt.plot(ts, trader_stats[\"cum_costs\"], label=\"Expert policy\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Transaction fees (USDT)\")\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "if SAVE_FIG:\n",
    "    plt.savefig(os.path.join(PATH_FIGURES, \"AIRL_seeds_fees.pdf\"))\n",
    "plt.show()\n",
    "print(\"Total fees: \", airl_trader_stats[\"cum_costs\"][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 7: Appendix (data analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Limit order book data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange = \"BIT.COM\"\n",
    "symbol = \"SOL-USDT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "start_date = datetime(2023, 9, 1)\n",
    "end_date = datetime(2023, 9, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of dates\n",
    "dates = get_list_of_dates_between(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "prefix = \"order_book\"\n",
    "for date in dates:\n",
    "    file_name = (\n",
    "        f\"{exchange}_{symbol}_{prefix}_{date.strftime('%Y_%m_%d')}.parquet\"\n",
    "    )\n",
    "    df_single = pd.read_parquet(os.path.join(PATH, file_name))\n",
    "    if date == start_date:\n",
    "        df = df_single\n",
    "    else:\n",
    "        df = pd.concat([df, df_single])\n",
    "\n",
    "df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"mid_price\"] = (df[\"bid_0_price\"] + df[\"ask_0_price\"]) / 2\n",
    "\n",
    "for i in range(3):\n",
    "    df[f\"spread_{i}\"] = df[f\"ask_{i}_price\"] - df[f\"bid_{i}_price\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mid-price and returns analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the mid-price evolution\n",
    "plt.figure(figsize=FIG_SIZE)\n",
    "plt.plot(df[\"mid_price\"])\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Price (USDT)\")\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=3))\n",
    "plt.tight_layout()\n",
    "if SAVE_FIG:\n",
    "    plt.savefig(os.path.join(PATH_FIGURES, f\"{symbol}_mid_price.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"mid_price\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spread analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid of subplots\n",
    "fig, axs = plt.subplots(3, 1, figsize=(FIG_SIZE[0], FIG_SIZE[0]), sharey=False)\n",
    "\n",
    "# Plot the bid-ask spread evolution for each level\n",
    "for i in range(3):\n",
    "    axs[i].plot(df[f\"spread_{i}\"])\n",
    "    axs[i].set_xlabel(\"Time\")\n",
    "    axs[i].set_ylabel(f\"Spread on level {i+1}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "if SAVE_FIG:\n",
    "    fig.savefig(f\"{PATH_FIGURES}/{exchange}_{symbol}_lob_spreads.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Merge into a table\n",
    "\n",
    "# Describe the spread\n",
    "for i in range(3):\n",
    "    print(f\"Spread on level {i+1}\")\n",
    "    print(df[f\"spread_{i}\"].describe())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot with three subfigures with the best bid volumes\n",
    "fig, axs = plt.subplots(3, 1, figsize=(FIG_SIZE[0], FIG_SIZE[0]), sharey=True)\n",
    "for i in range(3):\n",
    "    axs[i].plot(df[f\"bid_{i}_size\"], color=COLOR_GREEN)\n",
    "    axs[i].set_ylabel(f\"Level {i+1} volume\")\n",
    "    axs[i].set_xlabel(\"Time\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "if SAVE_FIG:\n",
    "    fig.savefig(f\"{PATH_FIGURES}/{exchange}_{symbol}_lob_bid_volumes.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(f\"Level {i} volume statistics\")\n",
    "    # Show descriptive statistics in non-scientific notation\n",
    "    pd.options.display.float_format = \"{:.3f}\".format\n",
    "    print(df[f\"bid_{i}_size\"].describe())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot with five subfigures containing histograms of the best bid volumes\n",
    "fig, axs = plt.subplots(3, 1, figsize=(FIG_SIZE[0], FIG_SIZE[0]))\n",
    "for i in range(3):\n",
    "    axs[i].hist(\n",
    "        df[f\"bid_{i}_size\"],\n",
    "        bins=100,\n",
    "        edgecolor=\"black\",\n",
    "        log=True,\n",
    "        color=COLOR_GREEN,\n",
    "        linewidth=0.3,\n",
    "    )\n",
    "    axs[i].set_ylabel(f\"Level {i+1} volume\")\n",
    "    axs[i].set_xlabel(\"Volume\")\n",
    "\n",
    "# Compute max volume for each level\n",
    "max_volumes = [df[f\"bid_{i}_size\"].max() for i in range(3)]\n",
    "max_volume = max(max_volumes)\n",
    "\n",
    "# Set the same x-axis and bins  for all subplots\n",
    "for i in range(3):\n",
    "    axs[i].set_xlim(0, max_volume)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "if SAVE_FIG:\n",
    "    fig.savefig(f\"{PATH_FIGURES}/{exchange}_{symbol}_lob_bid_volumes_hist.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot with five subfigures with the best ask volumes\n",
    "fig, axs = plt.subplots(3, 1, figsize=(FIG_SIZE[0], FIG_SIZE[0]))\n",
    "for i in range(3):\n",
    "    axs[i].plot(df[f\"ask_{i}_size\"], color=COLOR_RED)\n",
    "    axs[i].set_ylabel(f\"Level {i+1} volume\")\n",
    "    axs[i].set_xlabel(\"Time\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "if SAVE_FIG:\n",
    "    fig.savefig(f\"{PATH_FIGURES}/{exchange}_{symbol}_lob_ask_volumes.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(f\"Level {i} volume statistics\")\n",
    "    print(df[f\"ask_{i}_size\"].describe())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot with five subfigures containing histograms of the best bid volumes\n",
    "fig, axs = plt.subplots(3, 1, figsize=(FIG_SIZE[0], FIG_SIZE[0]), sharey=True)\n",
    "for i in range(3):\n",
    "    axs[i].hist(\n",
    "        df[f\"ask_{i}_size\"],\n",
    "        bins=100,\n",
    "        edgecolor=\"black\",\n",
    "        log=True,\n",
    "        color=COLOR_RED,\n",
    "    )\n",
    "    axs[i].set_ylabel(f\"Level {i+1} volume\")\n",
    "    axs[i].set_xlabel(\"Volume\")\n",
    "\n",
    "# Compute max volume for each level\n",
    "max_volumes = [df[f\"ask_{i}_size\"].max() for i in range(3)]\n",
    "max_volume = max(max_volumes)\n",
    "\n",
    "# Set the same x-axis and bins  for all subplots\n",
    "for i in range(3):\n",
    "    axs[i].set_xlim(0, max_volume)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "if SAVE_FIG:\n",
    "    fig.savefig(f\"{PATH_FIGURES}/{exchange}_{symbol}_lob_ask_volumes_hist.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order book imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the total volume at each level\n",
    "df[\"bid_total_volume\"] = 0\n",
    "df[\"ask_total_volume\"] = 0\n",
    "for i in range(20):\n",
    "    temp_bid_size = df[f\"bid_{i}_size\"]\n",
    "    temp_ask_size = df[f\"ask_{i}_size\"]\n",
    "    temp_bid_size = temp_bid_size.fillna(0)\n",
    "    temp_ask_size = temp_ask_size.fillna(0)\n",
    "    df[\"bid_total_volume\"] += temp_bid_size\n",
    "    df[\"ask_total_volume\"] += temp_ask_size\n",
    "\n",
    "df[\"imbalance\"] = (df[\"bid_total_volume\"] - df[\"ask_total_volume\"]) / (\n",
    "    df[\"bid_total_volume\"] + df[\"ask_total_volume\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the imbalance evolution\n",
    "ts_start = datetime(2023, 9, 11, 9, 0, 0)\n",
    "ts_end = datetime(2023, 9, 11, 12, 0, 0)\n",
    "\n",
    "fig = plt.figure(figsize=FIG_SIZE)\n",
    "# plt.plot(df[\"imbalance\"][start_index:max_index], color=\"black\")\n",
    "plt.plot(df[\"imbalance\"][ts_start:ts_end], color=\"black\")\n",
    "# Show only hours and minutes in the x-axis\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\"%H:%M\"))\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Volume imbalance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "if SAVE_FIG:\n",
    "    fig.savefig(f\"{PATH_FIGURES}/{exchange}_{symbol}_lob_volume_imbalance.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the imbalance signal for each level\n",
    "for i in range(20):\n",
    "    df[f\"imbalance_{i}\"] = (df[f\"bid_{i}_size\"] - df[f\"ask_{i}_size\"]) / (\n",
    "        df[f\"bid_{i}_size\"] + df[f\"ask_{i}_size\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the imbalance signal for top 5 levels\n",
    "ts_start = datetime(2023, 9, 11, 9, 0, 0)\n",
    "ts_end = datetime(2023, 9, 11, 12, 0, 0)\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(FIG_SIZE[0], FIG_SIZE[0]), sharey=True)\n",
    "\n",
    "for i in range(3):\n",
    "    axs[i].plot(df[f\"imbalance_{i}\"][ts_start:ts_end], color=\"black\")\n",
    "    axs[i].set_ylabel(f\"Level {i+1} imbalance\")\n",
    "    axs[i].xaxis.set_major_formatter(mdates.DateFormatter(\"%H:%M\"))\n",
    "    axs[i].set_xlabel(\"Time\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "if SAVE_FIG:\n",
    "    fig.savefig(f\"{PATH_FIGURES}/{exchange}_{symbol}_lob_level_imbalance.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orderbook snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the timestamps\n",
    "all_timestamps = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the index of the first timestamp that is larger than the given timestamp\n",
    "def find_first_index_larger_than(timestamp: datetime) -> int:\n",
    "    \"\"\"\n",
    "    Find the index of the first timestamp that is larger than the given\n",
    "    timestamp.\n",
    "    \"\"\"\n",
    "    for i, ts in enumerate(all_timestamps):\n",
    "        if ts > timestamp:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = find_first_index_larger_than(datetime(2023, 9, 9, 12, 4, 46))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 6\n",
    "index_start = index\n",
    "index_end = index_start + 1\n",
    "\n",
    "for i in range(index_start, index_end):\n",
    "    ts = all_timestamps[i]\n",
    "    bid_prices_labels = [f\"bid_{i}_price\" for i in range(depth)]\n",
    "    ask_prices_labels = [f\"ask_{i}_price\" for i in range(depth)]\n",
    "    bid_sizes_labels = [f\"bid_{i}_size\" for i in range(depth)]\n",
    "    ask_sizes_labels = [f\"ask_{i}_size\" for i in range(depth)]\n",
    "\n",
    "    # Process for one timestamp\n",
    "    row = df.loc[ts]\n",
    "    bid_prices = row[bid_prices_labels].to_numpy().flatten()\n",
    "    ask_prices = row[ask_prices_labels].to_numpy().flatten()\n",
    "    bid_volumes = row[bid_sizes_labels].to_numpy().cumsum()\n",
    "    ask_volumes = row[ask_sizes_labels].to_numpy().cumsum()\n",
    "\n",
    "    # X-axis\n",
    "    spread_space = 1\n",
    "    x_axis = np.arange(0, 2 * depth + spread_space, 1)\n",
    "\n",
    "    # Plot the order book snapshot\n",
    "    fig = plt.figure(figsize=FIG_SIZE)\n",
    "    plt.bar(\n",
    "        x_axis[:depth],\n",
    "        bid_volumes[::-1],\n",
    "        label=\"Bid\",\n",
    "        color=\"#9ED166\",\n",
    "        width=1,\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=1.3,\n",
    "    )\n",
    "    plt.bar(\n",
    "        x_axis[depth + spread_space :],\n",
    "        ask_volumes,\n",
    "        label=\"Ask\",\n",
    "        color=\"#EB735F\",\n",
    "        width=1,\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=1.3,\n",
    "    )\n",
    "    x_ticks = np.append(bid_prices[::-1], ask_prices)\n",
    "    x_ticks = np.insert(x_ticks, depth, \"\")\n",
    "    plt.xticks(x_axis, x_ticks, rotation=45, size=12)\n",
    "\n",
    "    plt.title(f\"Order book at {ts.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    plt.xlabel(\"Price\")\n",
    "    plt.ylabel(\"Volume\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Save the figure\n",
    "    if SAVE_FIG:\n",
    "        ts_str = ts.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "        fig.savefig(f\"{PATH_FIGURES}/{exchange}_{symbol}_lob_{ts_str}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Trade data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange = \"BIT.COM\"\n",
    "symbol = \"SOL-USDT\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load multiple dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "start_date = datetime(2023, 9, 1)\n",
    "end_date = datetime(2023, 9, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of dates\n",
    "dates = get_list_of_dates_between(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "prefix = \"trades\"\n",
    "for date in dates:\n",
    "    file_name = (\n",
    "        f\"{exchange}_{symbol}_{prefix}_{date.strftime('%Y_%m_%d')}.parquet\"\n",
    "    )\n",
    "    # df_single = pl.read_parquet(os.path.join(path, file_name))\n",
    "    df_single = pd.read_parquet(os.path.join(PATH, file_name))\n",
    "    if date == start_date:\n",
    "        df = df_single\n",
    "    else:\n",
    "        df = pd.concat([df, df_single])\n",
    "\n",
    "df.set_index(\"received_time\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data for each day\n",
    "avg_buy_volume = 0\n",
    "avg_sell_volume = 0\n",
    "avg_buy_orders = 0\n",
    "avg_sell_orders = 0\n",
    "\n",
    "prefix = \"trades\"\n",
    "for date in dates:\n",
    "    file_name = (\n",
    "        f\"{exchange}_{symbol}_{prefix}_{date.strftime('%Y_%m_%d')}.parquet\"\n",
    "    )\n",
    "    # df_single = pl.read_parquet(os.path.join(path, file_name))\n",
    "    df_single = pd.read_parquet(os.path.join(PATH, file_name))\n",
    "    print(f\"Statistics for date: {date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "    # Compute the number of buy and sell orders\n",
    "    buy_orders = df_single[df_single[\"side\"] == \"buy\"]\n",
    "    sell_orders = df_single[df_single[\"side\"] == \"sell\"]\n",
    "    avg_buy_orders += buy_orders.shape[0]\n",
    "    avg_sell_orders += sell_orders.shape[0]\n",
    "    print(\n",
    "        \"Number of buy | sell orders: \",\n",
    "        f\"{buy_orders.shape[0]} | {sell_orders.shape[0]}\",\n",
    "    )\n",
    "\n",
    "    # Compute the total volume of buy and sell orders\n",
    "    buy_volume = buy_orders[\"quantity\"].sum()\n",
    "    sell_volume = sell_orders[\"quantity\"].sum()\n",
    "    avg_buy_volume += buy_volume\n",
    "    avg_sell_volume += sell_volume\n",
    "    print(\n",
    "        \"Total buy | sell volume: \",\n",
    "        f\"{round(buy_volume, 2)} SOL | {round(sell_volume, 2)} SOL\",\n",
    "    )\n",
    "\n",
    "    # Compute the total volume\n",
    "    total_volume = df_single[\"quantity\"].sum()\n",
    "    print()\n",
    "\n",
    "# Compute the average number of buy and sell orders\n",
    "avg_buy_orders /= len(dates)\n",
    "avg_sell_orders /= len(dates)\n",
    "print(\"-\" * 50)\n",
    "print(f\"Average number of buy orders: {round(avg_buy_orders, 2)}\")\n",
    "print(f\"Average number of sell orders: {round(avg_sell_orders, 2)}\")\n",
    "\n",
    "# Compute the average buy and sell volume\n",
    "avg_buy_volume /= len(dates)\n",
    "avg_sell_volume /= len(dates)\n",
    "print(f\"Average buy volume: {round(avg_buy_volume, 2)}\")\n",
    "print(f\"Average sell volume: {round(avg_sell_volume, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize buy and sell volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot hours only instead of full timestamps\n",
    "date_format = DateFormatter(\"%H:%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the buy volumes\n",
    "fig = plt.figure(figsize=FIG_SIZE)\n",
    "plt.plot(buy_orders[\"quantity\"], color=COLOR_GREEN)\n",
    "# plt.gca().xaxis.set_major_formatter(date_format)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Volume (SOL)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# Save the figure\n",
    "if SAVE_FIG:\n",
    "    fig.savefig(f\"{PATH_FIGURES}/{exchange}_{symbol}_buy_volume.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the sell volumes\n",
    "fig = plt.figure(figsize=FIG_SIZE)\n",
    "plt.plot(sell_orders[\"quantity\"], color=COLOR_RED)\n",
    "# plt.gca().xaxis.set_major_formatter(date_format)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Volume (SOL)\")\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# Save the figure\n",
    "if SAVE_FIG:\n",
    "    fig.savefig(f\"{PATH_FIGURES}/{exchange}_{symbol}_sell_volume.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=FIG_SIZE)\n",
    "plt.hist(\n",
    "    buy_orders[\"quantity\"],\n",
    "    bins=100,\n",
    "    color=COLOR_GREEN,\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=1.1,\n",
    "    log=True,\n",
    ")\n",
    "plt.xlabel(\"Volume (SOL)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "if SAVE_FIG:\n",
    "    fig.savefig(f\"{PATH_FIGURES}/{exchange}_{symbol}_buy_volume_hist.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=FIG_SIZE)\n",
    "plt.hist(\n",
    "    sell_orders[\"quantity\"],\n",
    "    bins=100,\n",
    "    color=COLOR_RED,\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=1.1,\n",
    "    log=True,\n",
    ")\n",
    "plt.xlabel(\"Volume (SOL)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "if SAVE_FIG:\n",
    "    fig.savefig(f\"{PATH_FIGURES}/{exchange}_{symbol}_sell_volume_hist.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the buy orders statistics\n",
    "buy_orders[[\"quantity\", \"price\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the sell orders statistics\n",
    "sell_orders[[\"quantity\", \"price\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trade flow imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the data to 1 minute intervals\n",
    "buy_orders.set_index(\"received_time\", inplace=True)\n",
    "sell_orders.set_index(\"received_time\", inplace=True)\n",
    "buy_orders_1min = buy_orders[\"quantity\"].resample(\"1min\").sum()\n",
    "sell_order_1min = sell_orders[\"quantity\"].resample(\"1min\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the order flow imbalance\n",
    "eps = 1e-8\n",
    "denominator = buy_orders_1min + sell_order_1min\n",
    "denominator = denominator.replace(0, eps)\n",
    "imbalance = (buy_orders_1min - sell_order_1min) / denominator\n",
    "\n",
    "# Describe the order flow imbalance statistics\n",
    "imbalance.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trade flow imbalance\n",
    "start_index = 720\n",
    "end_index = 1080\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "plt.plot(imbalance[start_index:end_index], color=\"black\")\n",
    "plt.gca().xaxis.set_major_formatter(date_format)\n",
    "plt.xlabel(\"Time (hours)\")\n",
    "plt.ylabel(\"Trade flow imbalance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "if SAVE_FIG:\n",
    "    fig.savefig(f\"{PATH_FIGURES}/{exchange}_{symbol}_trade_flow_imbalance.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airl-market-making-BPwY8dy3-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
