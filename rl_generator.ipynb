{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning (generator)\n",
    "\n",
    "In this notebook, I implement a pure reinforcement learning agent. This is done to analyze the stability of training of the `generator` in the adversarial inverse reinforcement learning setting. I tested here various hyperparameters while using the perfect reward function (i.e. excluding the `discriminator` from inverse reinforcement learning) to gain better understanding of the generator's learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "import gymnasium as gym\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from stable_baselines3.ppo import PPO, MlpPolicy\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "from lob.traders import RLMarketMaker\n",
    "from lob.commissions import BitComCommissions, BinanceCommissions\n",
    "from lob.utils import  set_plot_style, get_lot_size, get_tick_size\n",
    "from rl.environments import LimitOrderBookGym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot style\n",
    "set_plot_style()\n",
    "\n",
    "# Set seed and random number generator\n",
    "SEED = 1\n",
    "RNG = np.random.default_rng(SEED)\n",
    "\n",
    "# Set device\n",
    "DEVICE = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Pandas display options (show all columns)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the market making agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "EXCHANGE_NAME = \"BIT.COM\" \n",
    "# EXCHANGE_NAME = \"BINANCE\"\n",
    "# EXCHANGE_NAME = \"OKX\"\n",
    "# EXCHANGE_NAME = \"GATEIO\"\n",
    "SYMBOL = \"SOL-USDT\"\n",
    "PATH = \"~/Projects/thesis-market-making/reinforcement-learning/data/\"\n",
    "ORDER_FLOW_PENALTY = 2 # Penalty for division of incoming order flow\n",
    "TICK_SIZE = get_tick_size(EXCHANGE_NAME) # Tick size of the limit order book\n",
    "LOT_SIZE = get_lot_size(EXCHANGE_NAME) # Lot size of the limit order book\n",
    "DEPTH = 20 # Depth of the data to load to the limit order book (max 20)\n",
    "EXCHANGE_TRADER_ID = \"Exchange\"\n",
    "MAX_STEPS = 300 # Maximum number of steps in an episode\n",
    "TS_START = pd.Timestamp(\"2023-09-01 00:00:00\") # Start of the episode\n",
    "TS_END = pd.Timestamp(\"2023-09-10 23:59:59\") # End of the episode\n",
    "DETERMINISTIC = False # Indicates whether to use a deterministic environment\n",
    "WIN = 0 # Window size for the features computation\n",
    "LOGGING = False # Indicates whether to log events\n",
    "TS_SAVE = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") # Ts for model saving\n",
    "traders = {} # Dictionary of traders\n",
    "\n",
    "print(\"Timestamp for saving: \", TS_SAVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters for the RL agent\n",
    "rl_trader_id = \"RLMarketMaker\"\n",
    "com_model = BinanceCommissions(tier=10)\n",
    "volume = 10\n",
    "# TODO: Update commissions and volume\n",
    "\n",
    "trader = RLMarketMaker(\n",
    "    id=rl_trader_id,\n",
    "    com_model=com_model,\n",
    "    volume=volume,\n",
    ")\n",
    "traders[rl_trader_id] = trader\n",
    "\n",
    "# Write a description of the experiment\n",
    "description = \"RL market maker simulation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the limit order book environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters for the environment\n",
    "ID = \"LimitOrderBookGym-v0\"\n",
    "ENTRY_POINT=LimitOrderBookGym\n",
    "KWARGS = {\n",
    "    \"exchange_name\": EXCHANGE_NAME,\n",
    "    \"symbol_name\": SYMBOL,\n",
    "    \"tick_size\": TICK_SIZE,\n",
    "    \"lot_size\": LOT_SIZE,\n",
    "    \"depth\": DEPTH,\n",
    "    \"order_flow_penalty\": ORDER_FLOW_PENALTY,\n",
    "    \"traders\": traders,\n",
    "    \"max_steps\": MAX_STEPS,\n",
    "    \"ts_start\": TS_START,\n",
    "    \"ts_end\": TS_END,\n",
    "    \"deterministic\": DETERMINISTIC,\n",
    "    \"win\": WIN,\n",
    "    \"path\": PATH,\n",
    "    \"rl_trader_id\": rl_trader_id,\n",
    "    \"logging\": LOGGING,\n",
    "    \"ts_save\": TS_SAVE,\n",
    "    \"description\": description,\n",
    "}\n",
    "\n",
    "# Register the environment\n",
    "gym.envs.register(\n",
    "    id=ID,\n",
    "    entry_point=ENTRY_POINT,\n",
    "    kwargs=KWARGS,\n",
    "    max_episode_steps=MAX_STEPS,\n",
    ")\n",
    "\n",
    "# Create the environment\n",
    "env = Monitor(gym.make(ID))\n",
    "check_env(env)\n",
    "env.reset()\n",
    "\n",
    "# Save the saving ts\n",
    "ts_save = env.unwrapped.exchange.ts_save\n",
    "print(f\"Saving ts: {ts_save}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Visualize the deterministic policy\n",
    "env.reset()\n",
    "terminated = False\n",
    "while not terminated:\n",
    "    # action = env.action_space.sample()  # this is where you would insert your policy\n",
    "    action = 12\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"Reward: {reward}\")\n",
    "    print()\n",
    "        \n",
    "    print(f\"Observation: {observation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define custom tensors and methods for better monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom tensors for monitoring \n",
    "monitor_states_orig = [\n",
    "    th.tensor([-1]),\n",
    "    th.tensor([-0.5]),\n",
    "    th.tensor([0]),\n",
    "    th.tensor([0.5]),\n",
    "    th.tensor([1]),\n",
    "]\n",
    "n_actions = 22\n",
    "monitor_actions_orig = [\n",
    "    th.tensor(x) for x in range(n_actions)\n",
    "]\n",
    "eye = th.eye(n_actions)\n",
    "monitor_actions_hot_orig = [eye[x] for x in range(n_actions)]\n",
    "\n",
    "monitor_states = th.stack(\n",
    "    [x for x in monitor_states_orig for _ in range(n_actions)]\n",
    ").to(DEVICE)\n",
    "monitor_actions = th.stack(\n",
    "    monitor_actions_orig * len(monitor_states_orig)\n",
    ").to(DEVICE)\n",
    "monitor_actions_hot = th.stack(\n",
    "    monitor_actions_hot_orig * len(monitor_states_orig)\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_probabilities(\n",
    "    model: PPO,\n",
    "    monitor_states: th.Tensor,\n",
    "    monitor_states_orig: list,\n",
    "    monitor_actions: th.Tensor,\n",
    "    n_actions: int,\n",
    ") -> pd.DataFrame:\n",
    "    _, logprobs_policy, _ = model.policy.evaluate_actions(\n",
    "        monitor_states,\n",
    "        monitor_actions,\n",
    "    )\n",
    "    probs_policy = th.exp(logprobs_policy).reshape(\n",
    "    len(monitor_states_orig), n_actions\n",
    "    )\n",
    "    probs_policy = np.hstack(\n",
    "        [\n",
    "            th.stack(monitor_states_orig).detach().numpy(),\n",
    "            probs_policy.cpu().detach().numpy(),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    df_probs_policy = pd.DataFrame(\n",
    "        probs_policy,\n",
    "        columns=[\"state\", *[f\"A{x}\" for x in range(n_actions)]],\n",
    "    )\n",
    "    df_probs_policy = df_probs_policy.round(2)\n",
    "\n",
    "    return df_probs_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the reinforcement learning agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters for the underlying policy\n",
    "learning_rate = 0.001           # Learning rate, can be a function of progress\n",
    "\n",
    "n_steps = 4500                  # Number of steps to run for each environment per update\n",
    "batch_size = 15                 # Mini batch size for each gradient update\n",
    "n_epochs = 10                   # Number of epoch when optimizing the surrogate loss\n",
    "\n",
    "gamma = 0                       # Discount factor\n",
    "gae_lambda = 0.95               # Generalized Advantage Estimator factor \n",
    "clip_range = 0.1                # Clipping parameter, can be a function of progress\n",
    "ent_coef = 0.01                 # Entropy coefficient for the loss calculation\n",
    "vf_coef = 0.5                   # Value function coefficient for the loss calculation\n",
    "max_grad_norm = 0.5             # The maximum value for the gradient clipping\n",
    "\n",
    "seed = SEED                     # Seed for the pseudo random generators\n",
    "verbose = 0                     # Verbosity level: 0 no output, 1 info, 2 debug\n",
    "normalize_advantage = True      # Whether to normalize or not the advantage\n",
    "\n",
    "clip_range_vf = None            # Clip for the value function, can be a func of progress\n",
    "use_sde = False                 # Whether to use State Dependent Exploration or not\n",
    "sde_sample_freq = -1            # Sample a new noise matrix every n steps (-1 = disable)\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the learner policy\n",
    "learner = PPO(\n",
    "    env=env,\n",
    "    policy=MlpPolicy,\n",
    "    learning_rate=learning_rate,\n",
    "    n_steps=n_steps,\n",
    "    batch_size=batch_size,\n",
    "    n_epochs=n_epochs,\n",
    "    gamma=gamma,\n",
    "    gae_lambda=gae_lambda,\n",
    "    clip_range=clip_range,\n",
    "    clip_range_vf=clip_range_vf,\n",
    "    normalize_advantage=normalize_advantage,\n",
    "    ent_coef=ent_coef,\n",
    "    vf_coef=vf_coef,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    use_sde=use_sde,\n",
    "    sde_sample_freq=sde_sample_freq,\n",
    "    verbose=verbose,\n",
    "    seed=seed,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the random policy\n",
    "mean_reward, std_reward = evaluate_policy(learner, env, n_eval_episodes=5, deterministic=False)\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Visualize the policy before training\n",
    "observation = env.reset()[0]\n",
    "\n",
    "terminated = False\n",
    "while not terminated:\n",
    "    # action = env.action_space.sample()  # this is where you would insert your policy\n",
    "    action, _ = learner.predict(observation, deterministic=True)\n",
    "    print(f\"Observation: {observation}\")\n",
    "    print(f\"Action: {action}\")\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"Reward: {reward}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the probabilities of states and actions\n",
    "probs = evaluate_probabilities(\n",
    "    model=learner,\n",
    "    monitor_states=monitor_states,\n",
    "    monitor_states_orig=monitor_states_orig,\n",
    "    monitor_actions=monitor_actions,\n",
    "    n_actions=n_actions,\n",
    ")\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = 3000\n",
    "\n",
    "# Train the agent\n",
    "for i in range(15):\n",
    "    # Train the agent for n steps\n",
    "    learner.learn(total_timesteps=train_steps, progress_bar=False)\n",
    "    \n",
    "    # Evaluate the probabilities of states and actions\n",
    "    probs = evaluate_probabilities(\n",
    "        model=learner,\n",
    "        monitor_states=monitor_states,\n",
    "        monitor_states_orig=monitor_states_orig,\n",
    "        monitor_actions=monitor_actions,\n",
    "        n_actions=n_actions,\n",
    "    )\n",
    "    print(\"Probabilities for iteration: \", i)\n",
    "    print(probs)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the trained agent\n",
    "mean_reward, std_reward = evaluate_policy(learner, env, n_eval_episodes=5)\n",
    "print(f\"Mean reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the probabilities of states and actions\n",
    "probs = evaluate_probabilities(\n",
    "        model=learner,\n",
    "        monitor_states=monitor_states,\n",
    "        monitor_states_orig=monitor_states_orig,\n",
    "        monitor_actions=monitor_actions,\n",
    "        n_actions=n_actions,\n",
    "    )\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Visualize the learned policy\n",
    "observation = env.reset()[0]\n",
    "\n",
    "terminated = False\n",
    "while not terminated:\n",
    "    # action = env.action_space.sample()  # this is where you would insert your policy\n",
    "    action, _ = learner.predict(observation, deterministic=True)\n",
    "    print(f\"Observation: {observation}\")\n",
    "    print(f\"Action: {action}\")\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"Reward: {reward}\")\n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simulator-rdDX6k4t-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
